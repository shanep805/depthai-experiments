{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Roboflow_Deploy_Custom_Mobilenet_to_OAK-D.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shanep805/depthai-experiments/blob/master/Roboflow_Deploy_Custom_Mobilenet_to_OAK_D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJz_ToJtkufM"
      },
      "source": [
        "# Roboflow and DepthAI Tutorial: Train and Deploy a custom object detection model with depth! \n",
        "\n",
        "#### ![Roboflow Workmark](https://i.imgur.com/WHFqYSJ.png)\n",
        "\n",
        "\n",
        "<img src=\"https://docs.luxonis.com/images/depthai_logo.png\" width=\"500\">\n",
        "\n",
        "We highly recommend working through this notebook with the corresponding blog post in hand. \n",
        "\n",
        "A Major shoutout to Luxonis, the creators of the OAK-D Device for originally putting this tutorial together. Here, we streamline the training process with data management, transformation, and exportation from Roboflow. \n",
        "\n",
        "At the end of this tutorial, you will have a custom trained object detection model to deploy to your OAK-D device!\n",
        "\n",
        "\n",
        "Insert GIF\n",
        "\n",
        "The steps we take to train and deploy our custom model are as follows \n",
        "\n",
        "\n",
        "* Install MobileNet Training Environment Dependencies\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TEgconcGvVK"
      },
      "source": [
        "# Install Training Environment Dependencies and Configs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH1x08R-yM-L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d17c851-815a-4f61-cb0c-488c8f64ae16"
      },
      "source": [
        "# %%capture\n",
        "#After this cell executes runtime will restart to finish the install, ignore and close the crash message, continue running cells starting with the one below\n",
        "!pip install numpy==1.17.5;\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy==1.17.5 in /usr/local/lib/python3.7/dist-packages (1.17.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9z8kgcs0E7iV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6cb6c05-06e2-4731-f8a1-4577e74cfdf6"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install tf_slim"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Requirement already satisfied: tf_slim in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from tf_slim) (0.12.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.2.2->tf_slim) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnNXNQCjdniL"
      },
      "source": [
        "\n",
        "# Number of training steps - 1000 will train very quickly, but more steps will increase accuracy.\n",
        "\n",
        "num_steps = 5000  # A step means using a single batch of data. larger batch, less steps required\n",
        "#60000 steps is required to train our example sign language dataset, less or more may be required based on your custom datasets needs\n",
        "\n",
        "#Number of evaluation steps.\n",
        "num_eval_steps = 50\n",
        "#Batch size 24 is a setting that generally works well. can be changed higher or lower \n",
        "MODELS_CONFIG = {\n",
        "        'ssd_mobilenet_v2': {\n",
        "        'model_name': 'ssd_mobilenet_v2_coco_2018_03_29',\n",
        "        'pipeline_file': 'ssd_mobilenet_v2_coco.config',\n",
        "        'batch_size': 24\n",
        "    }\n",
        "}\n",
        "selected_model = 'ssd_mobilenet_v2'\n",
        "\n",
        "# Name of the object detection model to use.\n",
        "MODEL = MODELS_CONFIG[selected_model]['model_name']\n",
        "\n",
        "# Name of the pipline file in tensorflow object detection API.\n",
        "pipeline_file = MODELS_CONFIG[selected_model]['pipeline_file']\n",
        "\n",
        "# Training batch size fits in Colab's GPU memory for selected model.\n",
        "batch_size = MODELS_CONFIG[selected_model]['batch_size']"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI8__uNS8-ns"
      },
      "source": [
        "# Install Tensorflow Object Detection API\n",
        "\n",
        "Clone TF models which contains the Object Detection API; also install the required dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecpHEnka8Kix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d2ab18c-3f75-41e2-b054-74faeb99e4bb"
      },
      "source": [
        "# %%capture\n",
        "%cd /content\n",
        "!git clone --quiet https://github.com/tensorflow/models.git\n",
        "%cd /content/models/\n",
        "!git checkout 58d19c67e1d30d905dd5c6e5092348658fed80af\n",
        "!apt-get update && apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n",
        "!pip install -q Cython contextlib2 pillow lxml matplotlib\n",
        "!pip install -q pycocotools\n",
        "%cd /content/models/research\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/slim/'\n",
        "!python object_detection/builders/model_builder_test.py"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "fatal: destination path 'models' already exists and is not an empty directory.\n",
            "/content/models\n",
            "HEAD is now at 58d19c67 Internal change\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Fetched 252 kB in 2s (109 kB/s)\n",
            "Reading package lists... Done\n",
            "/content/models/research\n",
            "object_detection/protos/input_reader.proto: warning: Import object_detection/protos/image_resizer.proto but not used.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT0asaLNN1Vz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "986896d6-19a3-4056-d8d6-500c779fc97f"
      },
      "source": [
        "import os\n",
        "\n",
        "repo_url = 'https://github.com/roboflow-ai/tensorflow-object-detection-faster-rcnn'\n",
        "\n",
        "%cd /content\n",
        "\n",
        "repo_dir_path = os.path.abspath(os.path.join('.', os.path.basename(repo_url)))\n",
        "\n",
        "!git clone {repo_url}\n",
        "%cd {repo_dir_path}\n",
        "!git pull\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "fatal: destination path 'tensorflow-object-detection-faster-rcnn' already exists and is not an empty directory.\n",
            "/content/tensorflow-object-detection-faster-rcnn\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-k7uGThXlny"
      },
      "source": [
        "## Prepare `tfrecord` files\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcJhUwgCIKpC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04017c68-f97d-43ac-c10d-7c5025f11ee5"
      },
      "source": [
        "%cd /content/tensorflow-object-detection-faster-rcnn/data\n",
        "\n",
        "!curl -L \"https://app.roboflow.com/ds/Q0wMvjSIZI?key=WnEvW0yGVu\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/tensorflow-object-detection-faster-rcnn/data\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   892  100   892    0     0    982      0 --:--:-- --:--:-- --:--:--   981\n",
            "100  141M  100  141M    0     0  55.9M      0  0:00:02  0:00:02 --:--:--  100M\n",
            "Archive:  roboflow.zip\n",
            "replace README.roboflow.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: All\n",
            " extracting: README.roboflow.txt     \n",
            " extracting: test/nk-wat-watches.tfrecord  \n",
            " extracting: test/nk-wat-watches_label_map.pbtxt  \n",
            " extracting: train/nk-wat-watches.tfrecord  \n",
            " extracting: train/nk-wat-watches_label_map.pbtxt  \n",
            " extracting: valid/nk-wat-watches.tfrecord  \n",
            " extracting: valid/nk-wat-watches_label_map.pbtxt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4K8uaumM9zo"
      },
      "source": [
        "#RENAME Based on your annotation type - In this example our annotation type is \"letters\"\n",
        "#RENAME to the file path based on your download above ^^\n",
        "test_record_fname = '/content/tensorflow-object-detection-faster-rcnn/data/test/nk-wat-watches.tfrecord'\n",
        "train_record_fname = '/content/tensorflow-object-detection-faster-rcnn/data/train/nk-wat-watches.tfrecord'\n",
        "label_map_pbtxt_fname = '/content/tensorflow-object-detection-faster-rcnn/data/train/nk-wat-watches_label_map.pbtxt'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzMxk7UWyS6g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39da1875-0dfd-4f2e-e386-15247e68ef1e"
      },
      "source": [
        "#double check that our class names came through\n",
        "#Rename based on your data above^^\n",
        "%cat '/content/tensorflow-object-detection-faster-rcnn/data/train/nk-wat-watches_label_map.pbtxt'"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "item {\n",
            "    name: \"NoGloves\",\n",
            "    id: 1,\n",
            "    display_name: \"NoGloves\"\n",
            "}\n",
            "item {\n",
            "    name: \"Tie\",\n",
            "    id: 2,\n",
            "    display_name: \"Tie\"\n",
            "}\n",
            "item {\n",
            "    name: \"bracelet\",\n",
            "    id: 3,\n",
            "    display_name: \"bracelet\"\n",
            "}\n",
            "item {\n",
            "    name: \"faceNoShield\",\n",
            "    id: 4,\n",
            "    display_name: \"faceNoShield\"\n",
            "}\n",
            "item {\n",
            "    name: \"faceSheildOn\",\n",
            "    id: 5,\n",
            "    display_name: \"faceSheildOn\"\n",
            "}\n",
            "item {\n",
            "    name: \"gloves\",\n",
            "    id: 6,\n",
            "    display_name: \"gloves\"\n",
            "}\n",
            "item {\n",
            "    name: \"longHair\",\n",
            "    id: 7,\n",
            "    display_name: \"longHair\"\n",
            "}\n",
            "item {\n",
            "    name: \"necklace\",\n",
            "    id: 8,\n",
            "    display_name: \"necklace\"\n",
            "}\n",
            "item {\n",
            "    name: \"ring\",\n",
            "    id: 9,\n",
            "    display_name: \"ring\"\n",
            "}\n",
            "item {\n",
            "    name: \"rubber gloves\",\n",
            "    id: 10,\n",
            "    display_name: \"rubber gloves\"\n",
            "}\n",
            "item {\n",
            "    name: \"scarf\",\n",
            "    id: 11,\n",
            "    display_name: \"scarf\"\n",
            "}\n",
            "item {\n",
            "    name: \"watch\",\n",
            "    id: 12,\n",
            "    display_name: \"watch\"\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MpYFYzNNumA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "809087de-9141-4e1f-c5e6-363a8355a82a"
      },
      "source": [
        "!ls '/content/tensorflow-object-detection-faster-rcnn/data/'"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FYI.txt  README.roboflow.txt  test  train  valid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCNYAaC7w6N8"
      },
      "source": [
        "## Download the Mobilenet SSD v2 Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orDCj6ihgUMR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69f957c6-1b8c-41ea-c2f7-25819bc88cdb"
      },
      "source": [
        "%cd /content/models/research\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import urllib.request\n",
        "import tarfile\n",
        "MODEL_FILE = MODEL + '.tar.gz'\n",
        "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
        "DEST_DIR = '/content/models/research/pretrained_model'\n",
        "\n",
        "if not (os.path.exists(MODEL_FILE)):\n",
        "    urllib.request.urlretrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
        "\n",
        "tar = tarfile.open(MODEL_FILE)\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "\n",
        "os.remove(MODEL_FILE)\n",
        "if (os.path.exists(DEST_DIR)):\n",
        "    shutil.rmtree(DEST_DIR)\n",
        "os.rename(MODEL, DEST_DIR)\n",
        "!echo {DEST_DIR}\n",
        "!ls -alh {DEST_DIR}"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research\n",
            "/content/models/research/pretrained_model\n",
            "total 135M\n",
            "drwxr-xr-x  3 345018 89939 4.0K Mar 30  2018 .\n",
            "drwxr-xr-x 63 root   root  4.0K May 29 20:25 ..\n",
            "-rw-r--r--  1 345018 89939   77 Mar 30  2018 checkpoint\n",
            "-rw-r--r--  1 345018 89939  67M Mar 30  2018 frozen_inference_graph.pb\n",
            "-rw-r--r--  1 345018 89939  65M Mar 30  2018 model.ckpt.data-00000-of-00001\n",
            "-rw-r--r--  1 345018 89939  15K Mar 30  2018 model.ckpt.index\n",
            "-rw-r--r--  1 345018 89939 3.4M Mar 30  2018 model.ckpt.meta\n",
            "-rw-r--r--  1 345018 89939 4.2K Mar 30  2018 pipeline.config\n",
            "drwxr-xr-x  3 345018 89939 4.0K Mar 30  2018 saved_model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHnxlfRznPP3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a4a44e24-7e78-481d-b0fd-952bb332bb51"
      },
      "source": [
        "#TF pretrained model checkpoint\n",
        "fine_tune_checkpoint = os.path.join(DEST_DIR, \"model.ckpt\")\n",
        "fine_tune_checkpoint"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/models/research/pretrained_model/model.ckpt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvwtHlLOeRJD"
      },
      "source": [
        "## Configuring a Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIhw7IdpLuiU"
      },
      "source": [
        "import os\n",
        "pipeline_fname = os.path.join('/content/models/research/object_detection/samples/configs/', pipeline_file)\n",
        "\n",
        "assert os.path.isfile(pipeline_fname), '`{}` not exist'.format(pipeline_fname)\n",
        "def get_num_classes(pbtxt_fname):\n",
        "    from object_detection.utils import label_map_util\n",
        "    label_map = label_map_util.load_labelmap(pbtxt_fname)\n",
        "    categories = label_map_util.convert_label_map_to_categories(\n",
        "        label_map, max_num_classes=90, use_display_name=True)\n",
        "    category_index = label_map_util.create_category_index(categories)\n",
        "    return len(category_index.keys())"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjtCbLF2i0wI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17fa02a8-1054-41fc-9b22-df449a69e800"
      },
      "source": [
        "import re\n",
        "iou_threshold = 0.50\n",
        "num_classes = get_num_classes(label_map_pbtxt_fname)\n",
        "with open(pipeline_fname) as f:\n",
        "    print(pipeline_fname)\n",
        "    s = f.read()\n",
        "with open(pipeline_fname, 'w') as f:\n",
        "    \n",
        "    # fine_tune_checkpoint\n",
        "    s = re.sub('fine_tune_checkpoint: \".*?\"',\n",
        "               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n",
        "    \n",
        "    # tfrecord files train and test.\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(train.record)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(val.record)(.*?\")', 'input_path: \"{}\"'.format(test_record_fname), s)\n",
        "\n",
        "    # label_map_path\n",
        "    s = re.sub(\n",
        "        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n",
        "\n",
        "    # Set training batch_size.\n",
        "    s = re.sub('batch_size: [0-9]+',\n",
        "               'batch_size: {}'.format(batch_size), s)\n",
        "\n",
        "    # Set training steps, num_steps\n",
        "    s = re.sub('num_steps: [0-9]+',\n",
        "               'num_steps: {}'.format(num_steps), s)\n",
        "    \n",
        "    # Set number of classes num_classes.\n",
        "    s = re.sub('num_classes: [0-9]+',\n",
        "               'num_classes: {}'.format(num_classes), s)\n",
        "    # Set number of classes num_classes.\n",
        "    s = re.sub('iou_threshold: [0-9].[0-9]+',\n",
        "               'iou_threshold: {}'.format(iou_threshold), s)\n",
        "    \n",
        "    f.write(s)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research/object_detection/samples/configs/ssd_mobilenet_v2_coco.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH0MEEanocn6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a215eaf-0d8b-4179-d711-8a93d432eff1"
      },
      "source": [
        "# #Have a look at the config file with various settings\n",
        "!cat {pipeline_fname}"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# SSD with Mobilenet v2 configuration for MSCOCO Dataset.\n",
            "# Users should configure the fine_tune_checkpoint field in the train config as\n",
            "# well as the label_map_path and input_path fields in the train_input_reader and\n",
            "# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\n",
            "# should be configured.\n",
            "\n",
            "model {\n",
            "  ssd {\n",
            "    num_classes: 12\n",
            "    box_coder {\n",
            "      faster_rcnn_box_coder {\n",
            "        y_scale: 10.0\n",
            "        x_scale: 10.0\n",
            "        height_scale: 5.0\n",
            "        width_scale: 5.0\n",
            "      }\n",
            "    }\n",
            "    matcher {\n",
            "      argmax_matcher {\n",
            "        matched_threshold: 0.5\n",
            "        unmatched_threshold: 0.5\n",
            "        ignore_thresholds: false\n",
            "        negatives_lower_than_unmatched: true\n",
            "        force_match_for_each_row: true\n",
            "      }\n",
            "    }\n",
            "    similarity_calculator {\n",
            "      iou_similarity {\n",
            "      }\n",
            "    }\n",
            "    anchor_generator {\n",
            "      ssd_anchor_generator {\n",
            "        num_layers: 6\n",
            "        min_scale: 0.2\n",
            "        max_scale: 0.95\n",
            "        aspect_ratios: 1.0\n",
            "        aspect_ratios: 2.0\n",
            "        aspect_ratios: 0.5\n",
            "        aspect_ratios: 3.0\n",
            "        aspect_ratios: 0.3333\n",
            "      }\n",
            "    }\n",
            "    image_resizer {\n",
            "      fixed_shape_resizer {\n",
            "        height: 300\n",
            "        width: 300\n",
            "      }\n",
            "    }\n",
            "    box_predictor {\n",
            "      convolutional_box_predictor {\n",
            "        min_depth: 0\n",
            "        max_depth: 0\n",
            "        num_layers_before_predictor: 0\n",
            "        use_dropout: false\n",
            "        dropout_keep_probability: 0.8\n",
            "        kernel_size: 1\n",
            "        box_code_size: 4\n",
            "        apply_sigmoid_to_scores: false\n",
            "        conv_hyperparams {\n",
            "          activation: RELU_6,\n",
            "          regularizer {\n",
            "            l2_regularizer {\n",
            "              weight: 0.00004\n",
            "            }\n",
            "          }\n",
            "          initializer {\n",
            "            truncated_normal_initializer {\n",
            "              stddev: 0.03\n",
            "              mean: 0.0\n",
            "            }\n",
            "          }\n",
            "          batch_norm {\n",
            "            train: true,\n",
            "            scale: true,\n",
            "            center: true,\n",
            "            decay: 0.9997,\n",
            "            epsilon: 0.001,\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    feature_extractor {\n",
            "      type: 'ssd_mobilenet_v2'\n",
            "      min_depth: 16\n",
            "      depth_multiplier: 1.0\n",
            "      conv_hyperparams {\n",
            "        activation: RELU_6,\n",
            "        regularizer {\n",
            "          l2_regularizer {\n",
            "            weight: 0.00004\n",
            "          }\n",
            "        }\n",
            "        initializer {\n",
            "          truncated_normal_initializer {\n",
            "            stddev: 0.03\n",
            "            mean: 0.0\n",
            "          }\n",
            "        }\n",
            "        batch_norm {\n",
            "          train: true,\n",
            "          scale: true,\n",
            "          center: true,\n",
            "          decay: 0.9997,\n",
            "          epsilon: 0.001,\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    loss {\n",
            "      classification_loss {\n",
            "        weighted_sigmoid {\n",
            "        }\n",
            "      }\n",
            "      localization_loss {\n",
            "        weighted_smooth_l1 {\n",
            "        }\n",
            "      }\n",
            "      hard_example_miner {\n",
            "        num_hard_examples: 3000\n",
            "        iou_threshold: 0.5\n",
            "        loss_type: CLASSIFICATION\n",
            "        max_negatives_per_positive: 3\n",
            "        min_negatives_per_image: 3\n",
            "      }\n",
            "      classification_weight: 1.0\n",
            "      localization_weight: 1.0\n",
            "    }\n",
            "    normalize_loss_by_num_matches: true\n",
            "    post_processing {\n",
            "      batch_non_max_suppression {\n",
            "        score_threshold: 1e-8\n",
            "        iou_threshold: 0.5\n",
            "        max_detections_per_class: 100\n",
            "        max_total_detections: 100\n",
            "      }\n",
            "      score_converter: SIGMOID\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "train_config: {\n",
            "  batch_size: 24\n",
            "  optimizer {\n",
            "    rms_prop_optimizer: {\n",
            "      learning_rate: {\n",
            "        exponential_decay_learning_rate {\n",
            "          initial_learning_rate: 0.004\n",
            "          decay_steps: 800720\n",
            "          decay_factor: 0.95\n",
            "        }\n",
            "      }\n",
            "      momentum_optimizer_value: 0.9\n",
            "      decay: 0.9\n",
            "      epsilon: 1.0\n",
            "    }\n",
            "  }\n",
            "  fine_tune_checkpoint: \"/content/models/research/pretrained_model/model.ckpt\"\n",
            "  fine_tune_checkpoint_type:  \"detection\"\n",
            "  # Note: The below line limits the training process to 200K steps, which we\n",
            "  # empirically found to be sufficient enough to train the pets dataset. This\n",
            "  # effectively bypasses the learning rate schedule (the learning rate will\n",
            "  # never decay). Remove the below line to train indefinitely.\n",
            "  num_steps: 5000\n",
            "  data_augmentation_options {\n",
            "    random_horizontal_flip {\n",
            "    }\n",
            "  }\n",
            "  data_augmentation_options {\n",
            "    ssd_random_crop {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "train_input_reader: {\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"/content/tensorflow-object-detection-faster-rcnn/data/train/nk-wat-watches.tfrecord\"\n",
            "  }\n",
            "  label_map_path: \"/content/tensorflow-object-detection-faster-rcnn/data/train/nk-wat-watches_label_map.pbtxt\"\n",
            "}\n",
            "\n",
            "eval_config: {\n",
            "  num_examples: 8000\n",
            "  # Note: The below line limits the evaluation process to 10 evaluations.\n",
            "  # Remove the below line to evaluate indefinitely.\n",
            "  max_evals: 10\n",
            "}\n",
            "\n",
            "eval_input_reader: {\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"/content/tensorflow-object-detection-faster-rcnn/data/test/nk-wat-watches.tfrecord\"\n",
            "  }\n",
            "  label_map_path: \"/content/tensorflow-object-detection-faster-rcnn/data/train/nk-wat-watches_label_map.pbtxt\"\n",
            "  shuffle: false\n",
            "  num_readers: 1\n",
            "}"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDddx2rPfex9"
      },
      "source": [
        "# Train Custom Mobilenet Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjDHjhKQofT5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df1281ea-57a7-49b5-cb7e-2caba66ac354"
      },
      "source": [
        "model_dir = 'training/'\n",
        "# Optionally remove content in output model directory for a fresh start.\n",
        "# !rm -rf {model_dir}\n",
        "# os.makedirs(model_dir, exist_ok=True)\n",
        "!python /content/models/research/object_detection/model_main.py \\\n",
        "    --pipeline_config_path={pipeline_fname} \\\n",
        "    --model_dir={model_dir} \\\n",
        "    --alsologtostderr \\\n",
        "    --num_train_steps={num_steps} \\\n",
        "    --num_eval_steps={num_eval_steps}\n",
        "\n",
        "#training will take a while (the TF OD library is not heavily optimized for speed on GPU), watch the mAP metrics rise, you can quit training early when you think your model has maxed out performance"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Forced number of epochs for all eval validations to be 1.\n",
            "W0529 20:25:28.816137 140111156799360 model_lib.py:717] Forced number of epochs for all eval validations to be 1.\n",
            "INFO:tensorflow:Maybe overwriting train_steps: 5000\n",
            "I0529 20:25:28.816384 140111156799360 config_util.py:552] Maybe overwriting train_steps: 5000\n",
            "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
            "I0529 20:25:28.816544 140111156799360 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
            "INFO:tensorflow:Maybe overwriting sample_1_of_n_eval_examples: 1\n",
            "I0529 20:25:28.816717 140111156799360 config_util.py:552] Maybe overwriting sample_1_of_n_eval_examples: 1\n",
            "INFO:tensorflow:Maybe overwriting eval_num_epochs: 1\n",
            "I0529 20:25:28.816907 140111156799360 config_util.py:552] Maybe overwriting eval_num_epochs: 1\n",
            "INFO:tensorflow:Maybe overwriting load_pretrained: True\n",
            "I0529 20:25:28.817044 140111156799360 config_util.py:552] Maybe overwriting load_pretrained: True\n",
            "INFO:tensorflow:Ignoring config override key: load_pretrained\n",
            "I0529 20:25:28.817181 140111156799360 config_util.py:562] Ignoring config override key: load_pretrained\n",
            "WARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
            "W0529 20:25:28.817469 140111156799360 model_lib.py:733] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
            "INFO:tensorflow:create_estimator_and_inputs: use_tpu False, export_to_tpu False\n",
            "I0529 20:25:28.817609 140111156799360 model_lib.py:768] create_estimator_and_inputs: use_tpu False, export_to_tpu False\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'training/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6dbf326290>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "I0529 20:25:28.818156 140111156799360 estimator.py:212] Using config: {'_model_dir': 'training/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6dbf326290>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "WARNING:tensorflow:Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7f6dbf066320>) includes params argument, but params are not passed to Estimator.\n",
            "W0529 20:25:28.818418 140111156799360 model_fn.py:630] Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7f6dbf066320>) includes params argument, but params are not passed to Estimator.\n",
            "INFO:tensorflow:Not using Distribute Coordinator.\n",
            "I0529 20:25:28.819009 140111156799360 estimator_training.py:186] Not using Distribute Coordinator.\n",
            "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
            "I0529 20:25:28.819278 140111156799360 training.py:612] Running training and evaluation locally (non-distributed).\n",
            "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
            "I0529 20:25:28.819577 140111156799360 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "W0529 20:25:28.833565 140111156799360 deprecation.py:323] From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
            "W0529 20:25:28.865752 140111156799360 dataset_builder.py:83] num_readers has been reduced to 1 to match input file shards.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "W0529 20:25:28.871489 140111156799360 deprecation.py:323] From /content/models/research/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:175: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W0529 20:25:28.893905 140111156799360 deprecation.py:323] From /content/models/research/object_detection/builders/dataset_builder.py:175: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "WARNING:tensorflow:Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f6dbf095210>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "W0529 20:25:28.936423 140111156799360 ag_logging.py:146] Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f6dbf095210>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "WARNING:tensorflow:Entity <function train_input.<locals>.transform_and_pad_input_data_fn at 0x7f6dbf0667a0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "W0529 20:25:29.180469 140111156799360 ag_logging.py:146] Entity <function train_input.<locals>.transform_and_pad_input_data_fn at 0x7f6dbf0667a0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/inputs.py:79: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "W0529 20:25:29.187080 140111156799360 deprecation.py:323] From /content/models/research/object_detection/inputs.py:79: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/utils/ops.py:493: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0529 20:25:29.196249 140111156799360 deprecation.py:323] From /content/models/research/object_detection/utils/ops.py:493: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/core/preprocessor.py:199: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "W0529 20:25:29.312685 140111156799360 deprecation.py:323] From /content/models/research/object_detection/core/preprocessor.py:199: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/inputs.py:260: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0529 20:25:30.258664 140111156799360 deprecation.py:323] From /content/models/research/object_detection/inputs.py:260: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0529 20:25:30.820461 140111156799360 estimator.py:1148] Calling model_fn.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "W0529 20:25:31.403180 140111156799360 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 20:25:34.551394 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 20:25:34.587636 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 20:25:34.623029 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 20:25:34.661806 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 20:25:34.697426 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 20:25:34.732815 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "W0529 20:25:34.782083 140111156799360 variables_helper.py:153] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 256, 512]], model variable shape: [[3, 3, 256, 512]]. This variable will not be initialized from the checkpoint.\n",
            "W0529 20:25:34.782335 140111156799360 variables_helper.py:153] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 128, 256]], model variable shape: [[3, 3, 128, 256]]. This variable will not be initialized from the checkpoint.\n",
            "W0529 20:25:34.782549 140111156799360 variables_helper.py:153] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 128, 256]], model variable shape: [[3, 3, 128, 256]]. This variable will not be initialized from the checkpoint.\n",
            "W0529 20:25:34.782747 140111156799360 variables_helper.py:153] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 64, 128]], model variable shape: [[3, 3, 64, 128]]. This variable will not be initialized from the checkpoint.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0529 20:25:43.481682 140111156799360 deprecation.py:506] From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0529 20:25:51.061276 140111156799360 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "I0529 20:25:51.062796 140111156799360 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0529 20:25:55.535671 140111156799360 monitored_session.py:240] Graph was finalized.\n",
            "2021-05-29 20:25:55.557192: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
            "2021-05-29 20:25:55.557525: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f8e6393d40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2021-05-29 20:25:55.557565: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2021-05-29 20:25:55.562243: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2021-05-29 20:25:55.848489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 20:25:55.849559: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f8f61988c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2021-05-29 20:25:55.849591: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2021-05-29 20:25:55.850876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 20:25:55.851802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-05-29 20:25:55.898007: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-05-29 20:25:56.087098: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-05-29 20:25:56.195987: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-05-29 20:25:56.225690: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-05-29 20:25:56.436792: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-05-29 20:25:56.458760: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-05-29 20:25:56.829632: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-05-29 20:25:56.829885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 20:25:56.830911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 20:25:56.831818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-05-29 20:25:56.835209: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-05-29 20:25:56.836987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-05-29 20:25:56.837021: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-05-29 20:25:56.837038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-05-29 20:25:56.837453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 20:25:56.838540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 20:25:56.839480: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-05-29 20:25:56.839547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15224 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0529 20:26:04.888252 140111156799360 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0529 20:26:05.325613 140111156799360 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into training/model.ckpt.\n",
            "I0529 20:26:17.926461 140111156799360 basic_session_run_hooks.py:606] Saving checkpoints for 0 into training/model.ckpt.\n",
            "2021-05-29 20:26:30.016350: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-05-29 20:26:32.427296: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "INFO:tensorflow:loss = 17.904741, step = 0\n",
            "I0529 20:26:34.129440 140111156799360 basic_session_run_hooks.py:262] loss = 17.904741, step = 0\n",
            "INFO:tensorflow:global_step/sec: 2.27589\n",
            "I0529 20:27:18.067461 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.27589\n",
            "INFO:tensorflow:loss = 6.882425, step = 100 (43.939 sec)\n",
            "I0529 20:27:18.068786 140111156799360 basic_session_run_hooks.py:260] loss = 6.882425, step = 100 (43.939 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.60043\n",
            "I0529 20:27:56.522665 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.60043\n",
            "INFO:tensorflow:loss = 4.1695957, step = 200 (38.455 sec)\n",
            "I0529 20:27:56.523850 140111156799360 basic_session_run_hooks.py:260] loss = 4.1695957, step = 200 (38.455 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.62399\n",
            "I0529 20:28:34.632673 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.62399\n",
            "INFO:tensorflow:loss = 3.8651416, step = 300 (38.110 sec)\n",
            "I0529 20:28:34.633825 140111156799360 basic_session_run_hooks.py:260] loss = 3.8651416, step = 300 (38.110 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.61982\n",
            "I0529 20:29:12.803329 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.61982\n",
            "INFO:tensorflow:loss = 3.7220166, step = 400 (38.171 sec)\n",
            "I0529 20:29:12.804640 140111156799360 basic_session_run_hooks.py:260] loss = 3.7220166, step = 400 (38.171 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.60936\n",
            "I0529 20:29:51.126786 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.60936\n",
            "INFO:tensorflow:loss = 2.972293, step = 500 (38.323 sec)\n",
            "I0529 20:29:51.128007 140111156799360 basic_session_run_hooks.py:260] loss = 2.972293, step = 500 (38.323 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.62102\n",
            "I0529 20:30:29.279817 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.62102\n",
            "INFO:tensorflow:loss = 3.1960416, step = 600 (38.153 sec)\n",
            "I0529 20:30:29.281105 140111156799360 basic_session_run_hooks.py:260] loss = 3.1960416, step = 600 (38.153 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.61212\n",
            "I0529 20:31:07.562911 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.61212\n",
            "INFO:tensorflow:loss = 3.1579447, step = 700 (38.283 sec)\n",
            "I0529 20:31:07.564155 140111156799360 basic_session_run_hooks.py:260] loss = 3.1579447, step = 700 (38.283 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.61455\n",
            "I0529 20:31:45.810451 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.61455\n",
            "INFO:tensorflow:loss = 2.4992957, step = 800 (38.247 sec)\n",
            "I0529 20:31:45.811386 140111156799360 basic_session_run_hooks.py:260] loss = 2.4992957, step = 800 (38.247 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.61816\n",
            "I0529 20:32:24.005217 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.61816\n",
            "INFO:tensorflow:loss = 3.0098867, step = 900 (38.195 sec)\n",
            "I0529 20:32:24.006392 140111156799360 basic_session_run_hooks.py:260] loss = 3.0098867, step = 900 (38.195 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.62542\n",
            "I0529 20:33:02.094330 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.62542\n",
            "INFO:tensorflow:loss = 2.8436553, step = 1000 (38.089 sec)\n",
            "I0529 20:33:02.095641 140111156799360 basic_session_run_hooks.py:260] loss = 2.8436553, step = 1000 (38.089 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.63174\n",
            "I0529 20:33:40.091971 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.63174\n",
            "INFO:tensorflow:loss = 2.4303694, step = 1100 (37.998 sec)\n",
            "I0529 20:33:40.093378 140111156799360 basic_session_run_hooks.py:260] loss = 2.4303694, step = 1100 (37.998 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.61546\n",
            "I0529 20:34:18.326185 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.61546\n",
            "INFO:tensorflow:loss = 2.4919376, step = 1200 (38.234 sec)\n",
            "I0529 20:34:18.327463 140111156799360 basic_session_run_hooks.py:260] loss = 2.4919376, step = 1200 (38.234 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.62266\n",
            "I0529 20:34:56.455362 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.62266\n",
            "INFO:tensorflow:loss = 2.872937, step = 1300 (38.129 sec)\n",
            "I0529 20:34:56.456863 140111156799360 basic_session_run_hooks.py:260] loss = 2.872937, step = 1300 (38.129 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.63442\n",
            "I0529 20:35:34.414419 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.63442\n",
            "INFO:tensorflow:loss = 2.232398, step = 1400 (37.959 sec)\n",
            "I0529 20:35:34.415731 140111156799360 basic_session_run_hooks.py:260] loss = 2.232398, step = 1400 (37.959 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.62904\n",
            "I0529 20:36:12.451140 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.62904\n",
            "INFO:tensorflow:loss = 2.4113033, step = 1500 (38.037 sec)\n",
            "I0529 20:36:12.452714 140111156799360 basic_session_run_hooks.py:260] loss = 2.4113033, step = 1500 (38.037 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1523 into training/model.ckpt.\n",
            "I0529 20:36:20.841483 140111156799360 basic_session_run_hooks.py:606] Saving checkpoints for 1523 into training/model.ckpt.\n",
            "WARNING:tensorflow:Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f6daa16d450>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "W0529 20:36:22.731676 140111156799360 ag_logging.py:146] Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f6daa16d450>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "WARNING:tensorflow:Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7f6db6d60b90> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "W0529 20:36:22.958646 140111156799360 ag_logging.py:146] Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7f6db6d60b90> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0529 20:36:23.564800 140111156799360 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 20:36:26.392866 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 20:36:26.445201 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 20:36:26.480777 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 20:36:26.517212 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 20:36:26.553231 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 20:36:26.589738 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/eval_util.py:830: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0529 20:36:27.840159 140111156799360 deprecation.py:323] From /content/models/research/object_detection/eval_util.py:830: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/utils/visualization_utils.py:618: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "W0529 20:36:28.089390 140111156799360 deprecation.py:323] From /content/models/research/object_detection/utils/visualization_utils.py:618: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0529 20:36:28.707517 140111156799360 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2021-05-29T20:36:28Z\n",
            "I0529 20:36:28.727071 140111156799360 evaluation.py:255] Starting evaluation at 2021-05-29T20:36:28Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0529 20:36:29.252295 140111156799360 monitored_session.py:240] Graph was finalized.\n",
            "2021-05-29 20:36:29.253435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 20:36:29.254197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-05-29 20:36:29.254287: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-05-29 20:36:29.254336: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-05-29 20:36:29.254382: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-05-29 20:36:29.254453: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-05-29 20:36:29.254510: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-05-29 20:36:29.254552: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-05-29 20:36:29.254616: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-05-29 20:36:29.254736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 20:36:29.255560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 20:36:29.256269: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-05-29 20:36:29.256328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-05-29 20:36:29.256349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-05-29 20:36:29.256366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-05-29 20:36:29.256492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 20:36:29.257287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 20:36:29.257915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15224 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from training/model.ckpt-1523\n",
            "I0529 20:36:29.259154 140111156799360 saver.py:1284] Restoring parameters from training/model.ckpt-1523\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0529 20:36:30.167940 140111156799360 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0529 20:36:30.295736 140111156799360 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 1233 images.\n",
            "I0529 20:37:25.574018 140107337963264 coco_evaluation.py:237] Performing evaluation on 1233 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0529 20:37:25.583439 140107337963264 coco_tools.py:116] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.13s)\n",
            "I0529 20:37:25.715596 140107337963264 coco_tools.py:138] DONE (t=0.13s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=12.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=1.55s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.056\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.169\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.017\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.009\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.057\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.068\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.056\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.100\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.109\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.028\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.116\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.112\n",
            "INFO:tensorflow:Finished evaluation at 2021-05-29-20:37:39\n",
            "I0529 20:37:39.808982 140111156799360 evaluation.py:275] Finished evaluation at 2021-05-29-20:37:39\n",
            "INFO:tensorflow:Saving dict for global step 1523: DetectionBoxes_Precision/mAP = 0.05586201, DetectionBoxes_Precision/mAP (large) = 0.06790609, DetectionBoxes_Precision/mAP (medium) = 0.05748583, DetectionBoxes_Precision/mAP (small) = 0.009306539, DetectionBoxes_Precision/mAP@.50IOU = 0.16944191, DetectionBoxes_Precision/mAP@.75IOU = 0.016571976, DetectionBoxes_Recall/AR@1 = 0.056034554, DetectionBoxes_Recall/AR@10 = 0.1002844, DetectionBoxes_Recall/AR@100 = 0.10898655, DetectionBoxes_Recall/AR@100 (large) = 0.111526, DetectionBoxes_Recall/AR@100 (medium) = 0.11553764, DetectionBoxes_Recall/AR@100 (small) = 0.027916303, Loss/classification_loss = 2.7111692, Loss/localization_loss = 0.9349413, Loss/regularization_loss = 0.2501652, Loss/total_loss = 3.89628, global_step = 1523, learning_rate = 0.004, loss = 3.89628\n",
            "I0529 20:37:39.809324 140111156799360 estimator.py:2049] Saving dict for global step 1523: DetectionBoxes_Precision/mAP = 0.05586201, DetectionBoxes_Precision/mAP (large) = 0.06790609, DetectionBoxes_Precision/mAP (medium) = 0.05748583, DetectionBoxes_Precision/mAP (small) = 0.009306539, DetectionBoxes_Precision/mAP@.50IOU = 0.16944191, DetectionBoxes_Precision/mAP@.75IOU = 0.016571976, DetectionBoxes_Recall/AR@1 = 0.056034554, DetectionBoxes_Recall/AR@10 = 0.1002844, DetectionBoxes_Recall/AR@100 = 0.10898655, DetectionBoxes_Recall/AR@100 (large) = 0.111526, DetectionBoxes_Recall/AR@100 (medium) = 0.11553764, DetectionBoxes_Recall/AR@100 (small) = 0.027916303, Loss/classification_loss = 2.7111692, Loss/localization_loss = 0.9349413, Loss/regularization_loss = 0.2501652, Loss/total_loss = 3.89628, global_step = 1523, learning_rate = 0.004, loss = 3.89628\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1523: training/model.ckpt-1523\n",
            "I0529 20:37:40.892385 140111156799360 estimator.py:2109] Saving 'checkpoint_path' summary for global step 1523: training/model.ckpt-1523\n",
            "INFO:tensorflow:global_step/sec: 0.846323\n",
            "I0529 20:38:10.609330 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 0.846323\n",
            "INFO:tensorflow:loss = 1.8996651, step = 1600 (118.158 sec)\n",
            "I0529 20:38:10.610629 140111156799360 basic_session_run_hooks.py:260] loss = 1.8996651, step = 1600 (118.158 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.61648\n",
            "I0529 20:38:48.828654 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.61648\n",
            "INFO:tensorflow:loss = 1.9908756, step = 1700 (38.219 sec)\n",
            "I0529 20:38:48.829870 140111156799360 basic_session_run_hooks.py:260] loss = 1.9908756, step = 1700 (38.219 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.6287\n",
            "I0529 20:39:26.870306 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.6287\n",
            "INFO:tensorflow:loss = 2.2727966, step = 1800 (38.042 sec)\n",
            "I0529 20:39:26.871569 140111156799360 basic_session_run_hooks.py:260] loss = 2.2727966, step = 1800 (38.042 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.62225\n",
            "I0529 20:40:05.005425 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.62225\n",
            "INFO:tensorflow:loss = 2.0012422, step = 1900 (38.135 sec)\n",
            "I0529 20:40:05.006754 140111156799360 basic_session_run_hooks.py:260] loss = 2.0012422, step = 1900 (38.135 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.62584\n",
            "I0529 20:40:43.088513 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.62584\n",
            "INFO:tensorflow:loss = 2.118874, step = 2000 (38.083 sec)\n",
            "I0529 20:40:43.089730 140111156799360 basic_session_run_hooks.py:260] loss = 2.118874, step = 2000 (38.083 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.62745\n",
            "I0529 20:41:21.148222 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.62745\n",
            "INFO:tensorflow:loss = 2.0744522, step = 2100 (38.060 sec)\n",
            "I0529 20:41:21.149300 140111156799360 basic_session_run_hooks.py:260] loss = 2.0744522, step = 2100 (38.060 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.63534\n",
            "I0529 20:41:59.094048 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.63534\n",
            "INFO:tensorflow:loss = 2.4851623, step = 2200 (37.946 sec)\n",
            "I0529 20:41:59.095240 140111156799360 basic_session_run_hooks.py:260] loss = 2.4851623, step = 2200 (37.946 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.63188\n",
            "I0529 20:42:37.089636 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.63188\n",
            "INFO:tensorflow:loss = 2.30562, step = 2300 (37.995 sec)\n",
            "I0529 20:42:37.090736 140111156799360 basic_session_run_hooks.py:260] loss = 2.30562, step = 2300 (37.995 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.62618\n",
            "I0529 20:43:15.167714 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.62618\n",
            "INFO:tensorflow:loss = 1.787923, step = 2400 (38.078 sec)\n",
            "I0529 20:43:15.169207 140111156799360 basic_session_run_hooks.py:260] loss = 1.787923, step = 2400 (38.078 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.64178\n",
            "I0529 20:43:53.021009 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.64178\n",
            "INFO:tensorflow:loss = 2.1853638, step = 2500 (37.853 sec)\n",
            "I0529 20:43:53.022059 140111156799360 basic_session_run_hooks.py:260] loss = 2.1853638, step = 2500 (37.853 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.64928\n",
            "I0529 20:44:30.767086 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.64928\n",
            "INFO:tensorflow:loss = 1.9101589, step = 2600 (37.746 sec)\n",
            "I0529 20:44:30.768321 140111156799360 basic_session_run_hooks.py:260] loss = 1.9101589, step = 2600 (37.746 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.65056\n",
            "I0529 20:45:08.494984 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.65056\n",
            "INFO:tensorflow:loss = 1.8826088, step = 2700 (37.728 sec)\n",
            "I0529 20:45:08.496170 140111156799360 basic_session_run_hooks.py:260] loss = 1.8826088, step = 2700 (37.728 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.64742\n",
            "I0529 20:45:46.267684 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.64742\n",
            "INFO:tensorflow:loss = 2.0344868, step = 2800 (37.773 sec)\n",
            "I0529 20:45:46.268986 140111156799360 basic_session_run_hooks.py:260] loss = 2.0344868, step = 2800 (37.773 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 2893 into training/model.ckpt.\n",
            "I0529 20:46:21.030668 140111156799360 basic_session_run_hooks.py:606] Saving checkpoints for 2893 into training/model.ckpt.\n",
            "WARNING:tensorflow:Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f6daa16de10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "W0529 20:46:22.848621 140111156799360 ag_logging.py:146] Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f6daa16de10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "WARNING:tensorflow:Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7f6c0bed5d40> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "W0529 20:46:23.054771 140111156799360 ag_logging.py:146] Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7f6c0bed5d40> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0529 20:46:23.665767 140111156799360 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 20:46:26.243118 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 20:46:26.280072 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 20:46:26.315752 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 20:46:26.352975 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 20:46:26.389060 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 20:46:26.424445 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0529 20:46:28.543728 140111156799360 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2021-05-29T20:46:28Z\n",
            "I0529 20:46:28.564929 140111156799360 evaluation.py:255] Starting evaluation at 2021-05-29T20:46:28Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0529 20:46:29.082677 140111156799360 monitored_session.py:240] Graph was finalized.\n",
            "2021-05-29 20:46:29.083500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 20:46:29.084206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-05-29 20:46:29.084319: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-05-29 20:46:29.084360: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-05-29 20:46:29.084419: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-05-29 20:46:29.084459: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-05-29 20:46:29.084512: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-05-29 20:46:29.084563: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-05-29 20:46:29.084600: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-05-29 20:46:29.084687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 20:46:29.085372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 20:46:29.085974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-05-29 20:46:29.086079: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-05-29 20:46:29.086104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-05-29 20:46:29.086119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-05-29 20:46:29.086269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 20:46:29.087010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 20:46:29.087718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15224 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from training/model.ckpt-2893\n",
            "I0529 20:46:29.088899 140111156799360 saver.py:1284] Restoring parameters from training/model.ckpt-2893\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0529 20:46:29.955557 140111156799360 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0529 20:46:30.077237 140111156799360 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 1233 images.\n",
            "I0529 20:47:24.331568 140107337963264 coco_evaluation.py:237] Performing evaluation on 1233 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0529 20:47:24.340125 140107337963264 coco_tools.py:116] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.14s)\n",
            "I0529 20:47:24.476689 140107337963264 coco_tools.py:138] DONE (t=0.14s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=12.69s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=1.48s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.072\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.205\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.025\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.015\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.072\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.087\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.066\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.122\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.129\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.036\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.129\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.138\n",
            "INFO:tensorflow:Finished evaluation at 2021-05-29-20:47:38\n",
            "I0529 20:47:38.963022 140111156799360 evaluation.py:275] Finished evaluation at 2021-05-29-20:47:38\n",
            "INFO:tensorflow:Saving dict for global step 2893: DetectionBoxes_Precision/mAP = 0.071779445, DetectionBoxes_Precision/mAP (large) = 0.08667128, DetectionBoxes_Precision/mAP (medium) = 0.07222034, DetectionBoxes_Precision/mAP (small) = 0.015310931, DetectionBoxes_Precision/mAP@.50IOU = 0.20498231, DetectionBoxes_Precision/mAP@.75IOU = 0.024689998, DetectionBoxes_Recall/AR@1 = 0.066115126, DetectionBoxes_Recall/AR@10 = 0.12150177, DetectionBoxes_Recall/AR@100 = 0.12860133, DetectionBoxes_Recall/AR@100 (large) = 0.13849868, DetectionBoxes_Recall/AR@100 (medium) = 0.12909697, DetectionBoxes_Recall/AR@100 (small) = 0.035707194, Loss/classification_loss = 2.3832464, Loss/localization_loss = 0.8354744, Loss/regularization_loss = 0.24949147, Loss/total_loss = 3.4682126, global_step = 2893, learning_rate = 0.004, loss = 3.4682126\n",
            "I0529 20:47:38.963306 140111156799360 estimator.py:2049] Saving dict for global step 2893: DetectionBoxes_Precision/mAP = 0.071779445, DetectionBoxes_Precision/mAP (large) = 0.08667128, DetectionBoxes_Precision/mAP (medium) = 0.07222034, DetectionBoxes_Precision/mAP (small) = 0.015310931, DetectionBoxes_Precision/mAP@.50IOU = 0.20498231, DetectionBoxes_Precision/mAP@.75IOU = 0.024689998, DetectionBoxes_Recall/AR@1 = 0.066115126, DetectionBoxes_Recall/AR@10 = 0.12150177, DetectionBoxes_Recall/AR@100 = 0.12860133, DetectionBoxes_Recall/AR@100 (large) = 0.13849868, DetectionBoxes_Recall/AR@100 (medium) = 0.12909697, DetectionBoxes_Recall/AR@100 (small) = 0.035707194, Loss/classification_loss = 2.3832464, Loss/localization_loss = 0.8354744, Loss/regularization_loss = 0.24949147, Loss/total_loss = 3.4682126, global_step = 2893, learning_rate = 0.004, loss = 3.4682126\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2893: training/model.ckpt-2893\n",
            "I0529 20:47:38.967604 140111156799360 estimator.py:2109] Saving 'checkpoint_path' summary for global step 2893: training/model.ckpt-2893\n",
            "INFO:tensorflow:global_step/sec: 0.863675\n",
            "I0529 20:47:42.051893 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 0.863675\n",
            "INFO:tensorflow:loss = 1.8142779, step = 2900 (115.784 sec)\n",
            "I0529 20:47:42.052979 140111156799360 basic_session_run_hooks.py:260] loss = 1.8142779, step = 2900 (115.784 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.63661\n",
            "I0529 20:48:19.979418 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.63661\n",
            "INFO:tensorflow:loss = 1.9469676, step = 3000 (37.928 sec)\n",
            "I0529 20:48:19.980488 140111156799360 basic_session_run_hooks.py:260] loss = 1.9469676, step = 3000 (37.928 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.63431\n",
            "I0529 20:48:57.940057 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.63431\n",
            "INFO:tensorflow:loss = 1.687316, step = 3100 (37.961 sec)\n",
            "I0529 20:48:57.941299 140111156799360 basic_session_run_hooks.py:260] loss = 1.687316, step = 3100 (37.961 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.64294\n",
            "I0529 20:49:35.776707 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.64294\n",
            "INFO:tensorflow:loss = 1.5738999, step = 3200 (37.837 sec)\n",
            "I0529 20:49:35.777935 140111156799360 basic_session_run_hooks.py:260] loss = 1.5738999, step = 3200 (37.837 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.63608\n",
            "I0529 20:50:13.711740 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.63608\n",
            "INFO:tensorflow:loss = 2.473855, step = 3300 (37.935 sec)\n",
            "I0529 20:50:13.712919 140111156799360 basic_session_run_hooks.py:260] loss = 2.473855, step = 3300 (37.935 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.64306\n",
            "I0529 20:50:51.546636 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.64306\n",
            "INFO:tensorflow:loss = 1.7822897, step = 3400 (37.835 sec)\n",
            "I0529 20:50:51.548011 140111156799360 basic_session_run_hooks.py:260] loss = 1.7822897, step = 3400 (37.835 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.6347\n",
            "I0529 20:51:29.501680 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.6347\n",
            "INFO:tensorflow:loss = 1.7796565, step = 3500 (37.955 sec)\n",
            "I0529 20:51:29.502755 140111156799360 basic_session_run_hooks.py:260] loss = 1.7796565, step = 3500 (37.955 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.64393\n",
            "I0529 20:52:07.324235 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.64393\n",
            "INFO:tensorflow:loss = 1.7440038, step = 3600 (37.823 sec)\n",
            "I0529 20:52:07.325383 140111156799360 basic_session_run_hooks.py:260] loss = 1.7440038, step = 3600 (37.823 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.6409\n",
            "I0529 20:52:45.190140 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.6409\n",
            "INFO:tensorflow:loss = 1.6812901, step = 3700 (37.866 sec)\n",
            "I0529 20:52:45.191369 140111156799360 basic_session_run_hooks.py:260] loss = 1.6812901, step = 3700 (37.866 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.65005\n",
            "I0529 20:53:22.925327 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.65005\n",
            "INFO:tensorflow:loss = 1.5644792, step = 3800 (37.735 sec)\n",
            "I0529 20:53:22.926699 140111156799360 basic_session_run_hooks.py:260] loss = 1.5644792, step = 3800 (37.735 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.63877\n",
            "I0529 20:54:00.821706 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.63877\n",
            "INFO:tensorflow:loss = 1.8706416, step = 3900 (37.896 sec)\n",
            "I0529 20:54:00.822729 140111156799360 basic_session_run_hooks.py:260] loss = 1.8706416, step = 3900 (37.896 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.64549\n",
            "I0529 20:54:38.621970 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.64549\n",
            "INFO:tensorflow:loss = 1.5585545, step = 4000 (37.801 sec)\n",
            "I0529 20:54:38.623313 140111156799360 basic_session_run_hooks.py:260] loss = 1.5585545, step = 4000 (37.801 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.63862\n",
            "I0529 20:55:16.520468 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.63862\n",
            "INFO:tensorflow:loss = 1.6605856, step = 4100 (37.898 sec)\n",
            "I0529 20:55:16.521713 140111156799360 basic_session_run_hooks.py:260] loss = 1.6605856, step = 4100 (37.898 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.65456\n",
            "I0529 20:55:54.191649 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.65456\n",
            "INFO:tensorflow:loss = 1.8409075, step = 4200 (37.671 sec)\n",
            "I0529 20:55:54.193156 140111156799360 basic_session_run_hooks.py:260] loss = 1.8409075, step = 4200 (37.671 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 4273 into training/model.ckpt.\n",
            "I0529 20:56:21.387921 140111156799360 basic_session_run_hooks.py:606] Saving checkpoints for 4273 into training/model.ckpt.\n",
            "WARNING:tensorflow:Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f6d381e86d0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "W0529 20:56:23.226211 140111156799360 ag_logging.py:146] Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f6d381e86d0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "WARNING:tensorflow:Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7f6c0c27bef0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "W0529 20:56:23.428885 140111156799360 ag_logging.py:146] Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7f6c0c27bef0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0529 20:56:24.036954 140111156799360 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 20:56:26.583908 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 20:56:26.618776 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 20:56:26.653337 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 20:56:26.692029 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 20:56:26.727250 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 20:56:26.761872 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0529 20:56:28.785567 140111156799360 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2021-05-29T20:56:28Z\n",
            "I0529 20:56:28.805807 140111156799360 evaluation.py:255] Starting evaluation at 2021-05-29T20:56:28Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0529 20:56:29.301492 140111156799360 monitored_session.py:240] Graph was finalized.\n",
            "2021-05-29 20:56:29.302298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 20:56:29.303170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-05-29 20:56:29.303274: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-05-29 20:56:29.303315: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-05-29 20:56:29.303373: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-05-29 20:56:29.303426: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-05-29 20:56:29.303477: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-05-29 20:56:29.303552: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-05-29 20:56:29.303619: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-05-29 20:56:29.303727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 20:56:29.304450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 20:56:29.305139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-05-29 20:56:29.305311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-05-29 20:56:29.305331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-05-29 20:56:29.305346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-05-29 20:56:29.305479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 20:56:29.306307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 20:56:29.306999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15224 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from training/model.ckpt-4273\n",
            "I0529 20:56:29.308078 140111156799360 saver.py:1284] Restoring parameters from training/model.ckpt-4273\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0529 20:56:30.225421 140111156799360 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0529 20:56:30.350155 140111156799360 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 1233 images.\n",
            "I0529 20:57:24.644965 140107329570560 coco_evaluation.py:237] Performing evaluation on 1233 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0529 20:57:24.653536 140107329570560 coco_tools.py:116] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.14s)\n",
            "I0529 20:57:24.790778 140107329570560 coco_tools.py:138] DONE (t=0.14s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=12.43s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=1.47s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.083\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.223\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.034\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.013\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.083\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.098\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.074\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.133\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.143\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.039\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.141\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.158\n",
            "INFO:tensorflow:Finished evaluation at 2021-05-29-20:57:39\n",
            "I0529 20:57:39.013705 140111156799360 evaluation.py:275] Finished evaluation at 2021-05-29-20:57:39\n",
            "INFO:tensorflow:Saving dict for global step 4273: DetectionBoxes_Precision/mAP = 0.083324015, DetectionBoxes_Precision/mAP (large) = 0.09799724, DetectionBoxes_Precision/mAP (medium) = 0.08287486, DetectionBoxes_Precision/mAP (small) = 0.01272372, DetectionBoxes_Precision/mAP@.50IOU = 0.22296575, DetectionBoxes_Precision/mAP@.75IOU = 0.033775892, DetectionBoxes_Recall/AR@1 = 0.074064836, DetectionBoxes_Recall/AR@10 = 0.13304433, DetectionBoxes_Recall/AR@100 = 0.14260946, DetectionBoxes_Recall/AR@100 (large) = 0.15782668, DetectionBoxes_Recall/AR@100 (medium) = 0.14143181, DetectionBoxes_Recall/AR@100 (small) = 0.039485335, Loss/classification_loss = 2.2340963, Loss/localization_loss = 0.7860186, Loss/regularization_loss = 0.24874638, Loss/total_loss = 3.2688572, global_step = 4273, learning_rate = 0.004, loss = 3.2688572\n",
            "I0529 20:57:39.014050 140111156799360 estimator.py:2049] Saving dict for global step 4273: DetectionBoxes_Precision/mAP = 0.083324015, DetectionBoxes_Precision/mAP (large) = 0.09799724, DetectionBoxes_Precision/mAP (medium) = 0.08287486, DetectionBoxes_Precision/mAP (small) = 0.01272372, DetectionBoxes_Precision/mAP@.50IOU = 0.22296575, DetectionBoxes_Precision/mAP@.75IOU = 0.033775892, DetectionBoxes_Recall/AR@1 = 0.074064836, DetectionBoxes_Recall/AR@10 = 0.13304433, DetectionBoxes_Recall/AR@100 = 0.14260946, DetectionBoxes_Recall/AR@100 (large) = 0.15782668, DetectionBoxes_Recall/AR@100 (medium) = 0.14143181, DetectionBoxes_Recall/AR@100 (small) = 0.039485335, Loss/classification_loss = 2.2340963, Loss/localization_loss = 0.7860186, Loss/regularization_loss = 0.24874638, Loss/total_loss = 3.2688572, global_step = 4273, learning_rate = 0.004, loss = 3.2688572\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4273: training/model.ckpt-4273\n",
            "I0529 20:57:39.018788 140111156799360 estimator.py:2109] Saving 'checkpoint_path' summary for global step 4273: training/model.ckpt-4273\n",
            "INFO:tensorflow:global_step/sec: 0.86601\n",
            "I0529 20:57:49.663705 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 0.86601\n",
            "INFO:tensorflow:loss = 1.7787374, step = 4300 (115.472 sec)\n",
            "I0529 20:57:49.664786 140111156799360 basic_session_run_hooks.py:260] loss = 1.7787374, step = 4300 (115.472 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.64731\n",
            "I0529 20:58:27.437967 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.64731\n",
            "INFO:tensorflow:loss = 2.0224226, step = 4400 (37.774 sec)\n",
            "I0529 20:58:27.439088 140111156799360 basic_session_run_hooks.py:260] loss = 2.0224226, step = 4400 (37.774 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.64901\n",
            "I0529 20:59:05.187919 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.64901\n",
            "INFO:tensorflow:loss = 1.653251, step = 4500 (37.750 sec)\n",
            "I0529 20:59:05.189312 140111156799360 basic_session_run_hooks.py:260] loss = 1.653251, step = 4500 (37.750 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.64345\n",
            "I0529 20:59:43.017248 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.64345\n",
            "INFO:tensorflow:loss = 1.4447402, step = 4600 (37.829 sec)\n",
            "I0529 20:59:43.018473 140111156799360 basic_session_run_hooks.py:260] loss = 1.4447402, step = 4600 (37.829 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.65243\n",
            "I0529 21:00:20.718457 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.65243\n",
            "INFO:tensorflow:loss = 1.5688955, step = 4700 (37.701 sec)\n",
            "I0529 21:00:20.719649 140111156799360 basic_session_run_hooks.py:260] loss = 1.5688955, step = 4700 (37.701 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.66035\n",
            "I0529 21:00:58.307470 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.66035\n",
            "INFO:tensorflow:loss = 1.5665499, step = 4800 (37.589 sec)\n",
            "I0529 21:00:58.309006 140111156799360 basic_session_run_hooks.py:260] loss = 1.5665499, step = 4800 (37.589 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.64512\n",
            "I0529 21:01:36.112874 140111156799360 basic_session_run_hooks.py:692] global_step/sec: 2.64512\n",
            "INFO:tensorflow:loss = 1.587809, step = 4900 (37.805 sec)\n",
            "I0529 21:01:36.114007 140111156799360 basic_session_run_hooks.py:260] loss = 1.587809, step = 4900 (37.805 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 5000 into training/model.ckpt.\n",
            "I0529 21:02:13.545550 140111156799360 basic_session_run_hooks.py:606] Saving checkpoints for 5000 into training/model.ckpt.\n",
            "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "I0529 21:02:15.291888 140111156799360 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "WARNING:tensorflow:Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f6d407e1610>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "W0529 21:02:15.360064 140111156799360 ag_logging.py:146] Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f6d407e1610>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "WARNING:tensorflow:Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7f6da018e5f0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "W0529 21:02:15.566387 140111156799360 ag_logging.py:146] Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7f6da018e5f0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0529 21:02:16.159401 140111156799360 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 21:02:18.657318 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 21:02:18.694977 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 21:02:18.741885 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 21:02:18.785053 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 21:02:18.827971 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 21:02:18.864476 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0529 21:02:21.297486 140111156799360 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2021-05-29T21:02:21Z\n",
            "I0529 21:02:21.316168 140111156799360 evaluation.py:255] Starting evaluation at 2021-05-29T21:02:21Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0529 21:02:21.817008 140111156799360 monitored_session.py:240] Graph was finalized.\n",
            "2021-05-29 21:02:21.817758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 21:02:21.818662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-05-29 21:02:21.818882: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-05-29 21:02:21.818941: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-05-29 21:02:21.819003: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-05-29 21:02:21.819043: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-05-29 21:02:21.819117: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-05-29 21:02:21.819171: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-05-29 21:02:21.819213: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-05-29 21:02:21.819298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 21:02:21.819962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 21:02:21.820581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-05-29 21:02:21.820650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-05-29 21:02:21.820669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-05-29 21:02:21.820691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-05-29 21:02:21.820817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 21:02:21.821475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 21:02:21.822093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15224 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from training/model.ckpt-5000\n",
            "I0529 21:02:21.823178 140111156799360 saver.py:1284] Restoring parameters from training/model.ckpt-5000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0529 21:02:22.749157 140111156799360 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0529 21:02:22.891219 140111156799360 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 1233 images.\n",
            "I0529 21:03:17.380765 140107337963264 coco_evaluation.py:237] Performing evaluation on 1233 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0529 21:03:17.388032 140107337963264 coco_tools.py:116] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.14s)\n",
            "I0529 21:03:17.528601 140107337963264 coco_tools.py:138] DONE (t=0.14s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=12.07s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=1.50s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.089\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.237\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.043\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.011\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.088\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.112\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.083\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.144\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.152\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.043\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.148\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.179\n",
            "INFO:tensorflow:Finished evaluation at 2021-05-29-21:03:31\n",
            "I0529 21:03:31.407726 140111156799360 evaluation.py:275] Finished evaluation at 2021-05-29-21:03:31\n",
            "INFO:tensorflow:Saving dict for global step 5000: DetectionBoxes_Precision/mAP = 0.089418255, DetectionBoxes_Precision/mAP (large) = 0.111654446, DetectionBoxes_Precision/mAP (medium) = 0.08793986, DetectionBoxes_Precision/mAP (small) = 0.011281617, DetectionBoxes_Precision/mAP@.50IOU = 0.23671016, DetectionBoxes_Precision/mAP@.75IOU = 0.043302845, DetectionBoxes_Recall/AR@1 = 0.082538515, DetectionBoxes_Recall/AR@10 = 0.14386442, DetectionBoxes_Recall/AR@100 = 0.15189502, DetectionBoxes_Recall/AR@100 (large) = 0.1786136, DetectionBoxes_Recall/AR@100 (medium) = 0.14791456, DetectionBoxes_Recall/AR@100 (small) = 0.043373622, Loss/classification_loss = 2.1829114, Loss/localization_loss = 0.7618928, Loss/regularization_loss = 0.24834245, Loss/total_loss = 3.1931474, global_step = 5000, learning_rate = 0.004, loss = 3.1931474\n",
            "I0529 21:03:31.408104 140111156799360 estimator.py:2049] Saving dict for global step 5000: DetectionBoxes_Precision/mAP = 0.089418255, DetectionBoxes_Precision/mAP (large) = 0.111654446, DetectionBoxes_Precision/mAP (medium) = 0.08793986, DetectionBoxes_Precision/mAP (small) = 0.011281617, DetectionBoxes_Precision/mAP@.50IOU = 0.23671016, DetectionBoxes_Precision/mAP@.75IOU = 0.043302845, DetectionBoxes_Recall/AR@1 = 0.082538515, DetectionBoxes_Recall/AR@10 = 0.14386442, DetectionBoxes_Recall/AR@100 = 0.15189502, DetectionBoxes_Recall/AR@100 (large) = 0.1786136, DetectionBoxes_Recall/AR@100 (medium) = 0.14791456, DetectionBoxes_Recall/AR@100 (small) = 0.043373622, Loss/classification_loss = 2.1829114, Loss/localization_loss = 0.7618928, Loss/regularization_loss = 0.24834245, Loss/total_loss = 3.1931474, global_step = 5000, learning_rate = 0.004, loss = 3.1931474\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5000: training/model.ckpt-5000\n",
            "I0529 21:03:31.412788 140111156799360 estimator.py:2109] Saving 'checkpoint_path' summary for global step 5000: training/model.ckpt-5000\n",
            "INFO:tensorflow:Performing the final export in the end of training.\n",
            "I0529 21:03:31.413563 140111156799360 exporter.py:410] Performing the final export in the end of training.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0529 21:03:31.717073 140111156799360 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 21:03:34.221618 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 21:03:34.258259 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 21:03:34.295164 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 21:03:34.332880 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 21:03:34.368702 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0529 21:03:34.404371 140111156799360 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0529 21:03:35.944827 140111156799360 estimator.py:1150] Done calling model_fn.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "W0529 21:03:35.945194 140111156799360 deprecation.py:323] From /tensorflow-1.15.2/python3.7/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
            "I0529 21:03:35.946136 140111156799360 export_utils.py:170] Signatures INCLUDED in export for Classify: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
            "I0529 21:03:35.946312 140111156799360 export_utils.py:170] Signatures INCLUDED in export for Regress: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['tensorflow/serving/predict', 'serving_default']\n",
            "I0529 21:03:35.946473 140111156799360 export_utils.py:170] Signatures INCLUDED in export for Predict: ['tensorflow/serving/predict', 'serving_default']\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
            "I0529 21:03:35.946668 140111156799360 export_utils.py:170] Signatures INCLUDED in export for Train: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
            "I0529 21:03:35.946787 140111156799360 export_utils.py:170] Signatures INCLUDED in export for Eval: None\n",
            "2021-05-29 21:03:35.947463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 21:03:35.948228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-05-29 21:03:35.948312: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-05-29 21:03:35.948352: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-05-29 21:03:35.948393: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-05-29 21:03:35.948448: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-05-29 21:03:35.948487: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-05-29 21:03:35.948525: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-05-29 21:03:35.948562: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-05-29 21:03:35.948675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 21:03:35.949372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 21:03:35.950184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-05-29 21:03:35.950236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-05-29 21:03:35.950256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-05-29 21:03:35.950272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-05-29 21:03:35.950381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 21:03:35.951157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-29 21:03:35.951796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15224 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from training/model.ckpt-5000\n",
            "I0529 21:03:35.954652 140111156799360 saver.py:1284] Restoring parameters from training/model.ckpt-5000\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "I0529 21:03:36.425985 140111156799360 builder_impl.py:665] Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "I0529 21:03:36.426245 140111156799360 builder_impl.py:460] No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: training/export/Servo/temp-b'1622322211'/saved_model.pb\n",
            "I0529 21:03:37.237347 140111156799360 builder_impl.py:425] SavedModel written to: training/export/Servo/temp-b'1622322211'/saved_model.pb\n",
            "INFO:tensorflow:Loss for final step: 1.5556847.\n",
            "I0529 21:03:37.669457 140111156799360 estimator.py:371] Loss for final step: 1.5556847.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KP-tUdtnRybs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9ee508e-cb24-4f98-ea2e-dbbca9f0564b"
      },
      "source": [
        "#model dir check for the trained model\n",
        "!ls {model_dir}"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint\n",
            "eval_0\n",
            "events.out.tfevents.1622319952.35c798ab2476\n",
            "export\n",
            "graph.pbtxt\n",
            "model.ckpt-0.data-00000-of-00001\n",
            "model.ckpt-0.index\n",
            "model.ckpt-0.meta\n",
            "model.ckpt-1523.data-00000-of-00001\n",
            "model.ckpt-1523.index\n",
            "model.ckpt-1523.meta\n",
            "model.ckpt-2893.data-00000-of-00001\n",
            "model.ckpt-2893.index\n",
            "model.ckpt-2893.meta\n",
            "model.ckpt-4273.data-00000-of-00001\n",
            "model.ckpt-4273.index\n",
            "model.ckpt-4273.meta\n",
            "model.ckpt-5000.data-00000-of-00001\n",
            "model.ckpt-5000.index\n",
            "model.ckpt-5000.meta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmSESMetj1sa"
      },
      "source": [
        "## Export a Trained Inference Graph\n",
        "Once your training job is complete, you need to extract the newly trained inference graph, which will be later used to perform the object detection. This can be done as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZJ_AF5MD3hZ"
      },
      "source": [
        "#clean output_directory if necessary to start fresh:\n",
        "\n",
        "# !rm -rf /content/object_detection_demo/fine_tuned_model/ \n",
        "# os.makedirs('/content/object_detection_demo_flow/fine_tuned_model/', exist_ok=True)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHoP90pUyKSq"
      },
      "source": [
        "%%capture\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "output_directory = './fine_tuned_model'\n",
        "# output_directory = '/content/gdrive/My\\ Drive/data/'\n",
        "\n",
        "lst = os.listdir(model_dir)\n",
        "lst = [l for l in lst if 'model.ckpt-' in l and '.meta' in l]\n",
        "steps=np.array([int(re.findall('\\d+', l)[0]) for l in lst])\n",
        "last_model = lst[steps.argmax()].replace('.meta', '')\n",
        "\n",
        "last_model_path = os.path.join(model_dir, last_model)\n",
        "print(last_model_path)\n",
        "!python /content/models/research/object_detection/export_inference_graph.py \\\n",
        "    --input_type=image_tensor \\\n",
        "    --pipeline_config_path={pipeline_fname} \\\n",
        "    --output_directory={output_directory} \\\n",
        "    --trained_checkpoint_prefix={last_model_path}"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usgBZvkz0nqD"
      },
      "source": [
        "#export directory check\n",
        "# !ls {output_directory}"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnDo1lonKgFr"
      },
      "source": [
        "import os\n",
        "pb_fname = os.path.join(os.path.abspath(output_directory), \"saved_model/saved_model.pb\")\n",
        "assert os.path.isfile(pb_fname), '`{}` not exist'.format(pb_fname)\n",
        "# !ls -alh {pb_fname}"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZHHqjjfMurV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "02f0eb15-ae6e-4efb-f678-aa8505844fbe"
      },
      "source": [
        "#download frozen graph for posterity, you can keep this so you don't have to start over on training\n",
        "from google.colab import files\n",
        "files.download(pb_fname)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_770f5973-ce92-4c1a-8a38-dd716ea98603\", \"saved_model.pb\", 20182127)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz1gX19GlVW7"
      },
      "source": [
        "## Running Inference: Checking what the trained model can detect\n",
        "Test with images in repository `tensorflow-object-detection-faster-rcnn/data/images/final test` directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iydNnHsvz5II"
      },
      "source": [
        "#You will need to import test images into the test folder specified below - In Roboflow you can export raw images in YOLO Darknet format\n",
        "\n",
        "test_folder = '/content/tensorflow-object-detection-faster-rcnn'\n",
        "sample_img = 'https://storage.googleapis.com/roboflow-platform-transforms/D7CAGZafFYZMvKbKibZGc42BUAx2/56694b0cb356d57b9da4914993042958/transformed.jpg'"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pzj9A4e5mj5l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9390f8e9-0794-40a8-9cee-215b9952ac55"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
        "PATH_TO_CKPT = pb_fname\n",
        "\n",
        "# List of the strings that is used to add correct label for each box.\n",
        "PATH_TO_LABELS = label_map_pbtxt_fname\n",
        "\n",
        "# If you want to test the code with your images, just add images files to the PATH_TO_TEST_IMAGES_DIR.\n",
        "PATH_TO_TEST_IMAGES_DIR = test_folder + \"/data/test/\"\n",
        "\n",
        "import urllib.request\n",
        "urllib.request.urlretrieve(sample_img, \n",
        "                           PATH_TO_TEST_IMAGES_DIR + \"language.jpg\")\n",
        "\n",
        "assert os.path.isfile(pb_fname)\n",
        "assert os.path.isfile(PATH_TO_LABELS)\n",
        "TEST_IMAGE_PATHS = glob.glob(os.path.join(PATH_TO_TEST_IMAGES_DIR, \"*.jpg*\"))\n",
        "assert len(TEST_IMAGE_PATHS) > 0, 'No image found in `{}`.'.format(PATH_TO_TEST_IMAGES_DIR)\n",
        "print(TEST_IMAGE_PATHS)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/content/tensorflow-object-detection-faster-rcnn/data/test/language.jpg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG5YUMdg1Po7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "746dcf78-24d1-4028-b8dc-8bff90a4d1fa"
      },
      "source": [
        "%cd /content/models/research/object_detection\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import six.moves.urllib as urllib\n",
        "import sys\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "# This is needed to display the images.\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# This is needed since the notebook is stored in the object_detection folder.\n",
        "sys.path.append(\"..\")\n",
        "from object_detection.utils import ops as utils_ops\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "\n",
        "\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "    od_graph_def = tf.GraphDef()\n",
        "    PATH_TO_CKPT = '/content/models/research/fine_tuned_model/frozen_inference_graph.pb'\n",
        "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "\n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(\n",
        "    label_map, max_num_classes=num_classes, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "\n",
        "def load_image_into_numpy_array(image):\n",
        "    (im_width, im_height) = image.size\n",
        "    return np.array(image.getdata()).reshape(\n",
        "        (im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "# Size, in inches, of the output images.\n",
        "IMAGE_SIZE = (12, 8)\n",
        "\n",
        "\n",
        "def run_inference_for_single_image(image, graph):\n",
        "    with graph.as_default():\n",
        "        with tf.Session() as sess:\n",
        "            # Get handles to input and output tensors\n",
        "            ops = tf.get_default_graph().get_operations()\n",
        "            all_tensor_names = {\n",
        "                output.name for op in ops for output in op.outputs}\n",
        "            tensor_dict = {}\n",
        "            for key in [\n",
        "                'num_detections', 'detection_boxes', 'detection_scores',\n",
        "                'detection_classes', 'detection_masks'\n",
        "            ]:\n",
        "                tensor_name = key + ':0'\n",
        "                if tensor_name in all_tensor_names:\n",
        "                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
        "                        tensor_name)\n",
        "            if 'detection_masks' in tensor_dict:\n",
        "                # The following processing is only for single image\n",
        "                detection_boxes = tf.squeeze(\n",
        "                    tensor_dict['detection_boxes'], [0])\n",
        "                detection_masks = tf.squeeze(\n",
        "                    tensor_dict['detection_masks'], [0])\n",
        "                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
        "                real_num_detection = tf.cast(\n",
        "                    tensor_dict['num_detections'][0], tf.int32)\n",
        "                detection_boxes = tf.slice(detection_boxes, [0, 0], [\n",
        "                                           real_num_detection, -1])\n",
        "                detection_masks = tf.slice(detection_masks, [0, 0, 0], [\n",
        "                                           real_num_detection, -1, -1])\n",
        "                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "                    detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
        "                detection_masks_reframed = tf.cast(\n",
        "                    tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
        "                # Follow the convention by adding back the batch dimension\n",
        "                tensor_dict['detection_masks'] = tf.expand_dims(\n",
        "                    detection_masks_reframed, 0)\n",
        "            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "            # Run inference\n",
        "            output_dict = sess.run(tensor_dict,\n",
        "                                   feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
        "\n",
        "            # all outputs are float32 numpy arrays, so convert types as appropriate\n",
        "            output_dict['num_detections'] = int(\n",
        "                output_dict['num_detections'][0])\n",
        "            output_dict['detection_classes'] = output_dict[\n",
        "                'detection_classes'][0].astype(np.uint8)\n",
        "            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
        "            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
        "            if 'detection_masks' in output_dict:\n",
        "                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
        "    return output_dict\n",
        "\n",
        "\n",
        "for image_path in TEST_IMAGE_PATHS:\n",
        "    image = Image.open(image_path)\n",
        "    print(image_path)\n",
        "    # the array based representation of the image will be used later in order to prepare the\n",
        "    # result image with boxes and labels on it.\n",
        "    image_np = load_image_into_numpy_array(image)\n",
        "    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
        "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "    # Actual detection.\n",
        "    output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
        "    # Visualization of the results of a detection.\n",
        "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "        image_np,\n",
        "        output_dict['detection_boxes'],\n",
        "        output_dict['detection_classes'],\n",
        "        output_dict['detection_scores'],\n",
        "        category_index,\n",
        "        instance_masks=output_dict.get('detection_masks'),\n",
        "        use_normalized_coordinates=True,\n",
        "        line_thickness=8)\n",
        "    plt.figure(figsize=IMAGE_SIZE)\n",
        "    plt.imshow(image_np)\n",
        "    plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research/object_detection\n",
            "/content/tensorflow-object-detection-faster-rcnn/data/test/language.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PlfZAR1OCK2"
      },
      "source": [
        "# Convert TF model to OpenVINO 20.01 Intermediate Representation (IR)\n",
        " This can be used to run inference on OpenVINO.\n",
        "# In order to run the model on DepthAI modules, we then compile the IR obtained above to a .blob (via a server we set up just for that) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qucQJwsWhL3"
      },
      "source": [
        "## First, we install Open Vino 20.01\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BxCxkQ6NkcF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "806b1d83-28d2-4461-aefd-359848ec58db"
      },
      "source": [
        "%cd ../.."
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5py5tfxS6VaA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b372314f-f74d-4bd7-a3ea-5df4a6bf5ebe"
      },
      "source": [
        "%%time\n",
        "# %%capture\n",
        "## install tools. Open Vino takes some time to download: 10-15 min sometimes.\n",
        "!sudo apt-get install -y pciutils cpio\n",
        "!sudo apt autoremove\n",
        "## downnload installation files\n",
        "!wget http://registrationcenter-download.intel.com/akdlm/irc_nas/16345/l_openvino_toolkit_p_2020.1.023.tgz\n",
        "path = \"l_openvino_toolkit_p_2020.1.023.tgz\"\n",
        "# path = \"/content/software/Intel OpenVINO 2019 R3.1/l_openvino_toolkit_p_2019.3.376.tgz\"\n",
        "## install openvino\n",
        "!tar xf \"{path}\"\n",
        "# !tar xf \"{path}\" && \\\n",
        "#     cd l_openvino_toolkit_p* && \\\n",
        "#     ./install_openvino_dependencies.sh && \\\n",
        "#     sed -i 's/decline/accept/g' silent.cfg && \\\n",
        "#     ./install.sh\n",
        "# ## install dependencies\n",
        "# !/opt/intel/openvino/install_dependencies/install_openvino_dependencies.sh\n",
        "# ## install prerequisites\n",
        "# !/opt/intel/openvino/deployment_tools/model_optimizer/install_prerequisites/install_prerequisites.sh"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libpci3\n",
            "Suggested packages:\n",
            "  libarchive1\n",
            "The following NEW packages will be installed:\n",
            "  cpio libpci3 pciutils\n",
            "0 upgraded, 3 newly installed, 0 to remove and 86 not upgraded.\n",
            "Need to get 368 kB of archives.\n",
            "After this operation, 1,786 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 cpio amd64 2.12+dfsg-6ubuntu0.18.04.1 [86.2 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpci3 amd64 1:3.5.2-1ubuntu1.1 [24.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 pciutils amd64 1:3.5.2-1ubuntu1.1 [257 kB]\n",
            "Fetched 368 kB in 2s (174 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package cpio.\n",
            "(Reading database ... 161091 files and directories currently installed.)\n",
            "Preparing to unpack .../cpio_2.12+dfsg-6ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking cpio (2.12+dfsg-6ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package libpci3:amd64.\n",
            "Preparing to unpack .../libpci3_1%3a3.5.2-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking libpci3:amd64 (1:3.5.2-1ubuntu1.1) ...\n",
            "Selecting previously unselected package pciutils.\n",
            "Preparing to unpack .../pciutils_1%3a3.5.2-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking pciutils (1:3.5.2-1ubuntu1.1) ...\n",
            "Setting up cpio (2.12+dfsg-6ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /bin/mt-gnu to provide /bin/mt (mt) in auto mode\n",
            "Setting up libpci3:amd64 (1:3.5.2-1ubuntu1.1) ...\n",
            "Setting up pciutils (1:3.5.2-1ubuntu1.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following packages will be REMOVED:\n",
            "  libnvidia-common-460\n",
            "0 upgraded, 0 newly installed, 1 to remove and 85 not upgraded.\n",
            "After this operation, 35.8 kB disk space will be freed.\n",
            "(Reading database ... 161122 files and directories currently installed.)\n",
            "Removing libnvidia-common-460 (460.73.01-0ubuntu1) ...\n",
            "--2021-05-29 21:08:24--  http://registrationcenter-download.intel.com/akdlm/irc_nas/16345/l_openvino_toolkit_p_2020.1.023.tgz\n",
            "Resolving registrationcenter-download.intel.com (registrationcenter-download.intel.com)... 23.199.34.226, 104.116.243.155\n",
            "Connecting to registrationcenter-download.intel.com (registrationcenter-download.intel.com)|23.199.34.226|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 508213676 (485M) [application/octet-stream]\n",
            "Saving to: l_openvino_toolkit_p_2020.1.023.tgz\n",
            "\n",
            "l_openvino_toolkit_ 100%[===================>] 484.67M  17.5MB/s    in 27s     \n",
            "\n",
            "2021-05-29 21:08:52 (17.8 MB/s) - l_openvino_toolkit_p_2020.1.023.tgz saved [508213676/508213676]\n",
            "\n",
            "CPU times: user 432 ms, sys: 206 ms, total: 638 ms\n",
            "Wall time: 43.9 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNFUanul7s1g"
      },
      "source": [
        "%%capture\n",
        "%cd l_openvino_toolkit_p_2020.1.023/\n",
        "!./install_openvino_dependencies.sh && \\\n",
        "    sed -i 's/decline/accept/g' silent.cfg && \\\n",
        "    ./install.sh --silent silent.cfg"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2enKxK_qakX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5672f397-0625-470c-832a-31aec61bd989"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EULA.txt\tinstall_openvino_dependencies.sh  pset\t\t  rpm\n",
            "install_GUI.sh\tinstall.sh\t\t\t  PUBLIC_KEY.PUB  silent.cfg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-jbjiw-7nAv"
      },
      "source": [
        "[Optional] Open Vino install check, generally not needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NJp28Oj7Ts9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e84c915-6a48-4789-a01e-ee044d3278a4"
      },
      "source": [
        "!source /opt/intel/openvino/bin/setupvars.sh && \\\n",
        "    /opt/intel/openvino/deployment_tools/demo/demo_squeezenet_download_convert_run.sh"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[setupvars.sh] OpenVINO environment initialized\n",
            "target_precision = FP16\n",
            "[setupvars.sh] OpenVINO environment initialized\n",
            "\n",
            "\n",
            "###################################################\n",
            "\n",
            "\n",
            "\n",
            "Downloading the Caffe model and the prototxt\n",
            "Installing dependencies\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Fetched 252 kB in 2s (128 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "85 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Run sudo -E apt -y install build-essential python3-pip virtualenv cmake libcairo2-dev libpango1.0-dev libglib2.0-dev libgtk2.0-dev libswscale-dev libavcodec-dev libavformat-dev libgstreamer1.0-0 gstreamer1.0-plugins-base\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.4ubuntu1).\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.1).\n",
            "gstreamer1.0-plugins-base is already the newest version (1.14.5-0ubuntu1~18.04.3).\n",
            "libglib2.0-dev is already the newest version (2.56.4-0ubuntu0.18.04.8).\n",
            "libglib2.0-dev set to manually installed.\n",
            "libgstreamer1.0-0 is already the newest version (1.14.5-0ubuntu1~18.04.2).\n",
            "libavcodec-dev is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "libavcodec-dev set to manually installed.\n",
            "libavformat-dev is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "libavformat-dev set to manually installed.\n",
            "libswscale-dev is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "libswscale-dev set to manually installed.\n",
            "The following additional packages will be installed:\n",
            "  autoconf automake autopoint autotools-dev debhelper dh-autoreconf\n",
            "  dh-strip-nondeterminism file gettext gettext-base gir1.2-atk-1.0\n",
            "  gir1.2-freedesktop gir1.2-gdkpixbuf-2.0 gir1.2-gtk-2.0 gir1.2-pango-1.0\n",
            "  intltool-debian libarchive-cpio-perl libarchive-zip-perl libatk1.0-dev\n",
            "  libcairo-script-interpreter2 libfile-stripnondeterminism-perl libgail-common\n",
            "  libgail18 libgdk-pixbuf2.0-dev libgtk2.0-0 libgtk2.0-bin libgtk2.0-common\n",
            "  libmagic-mgc libmagic1 libmail-sendmail-perl libpangoxft-1.0-0\n",
            "  libpixman-1-dev libsigsegv2 libsys-hostname-long-perl libtimedate-perl\n",
            "  libtool libxcb-shm0-dev libxcomposite-dev libxcursor-dev libxinerama-dev\n",
            "  libxml2-utils libxrandr-dev m4 po-debconf python-pip-whl python3-asn1crypto\n",
            "  python3-cffi-backend python3-crypto python3-cryptography python3-idna\n",
            "  python3-keyring python3-keyrings.alt python3-pkg-resources\n",
            "  python3-secretstorage python3-setuptools python3-six python3-virtualenv\n",
            "  python3-wheel python3-xdg x11proto-composite-dev x11proto-randr-dev\n",
            "  x11proto-xinerama-dev\n",
            "Suggested packages:\n",
            "  autoconf-archive gnu-standards autoconf-doc dh-make dwz gettext-doc\n",
            "  libasprintf-dev libgettextpo-dev libcairo2-doc gvfs libgtk2.0-doc\n",
            "  imagemagick libpango1.0-doc libtool-doc gcj-jdk m4-doc libmail-box-perl\n",
            "  python-crypto-doc python-cryptography-doc python3-cryptography-vectors\n",
            "  gnome-keyring libkf5wallet-bin gir1.2-gnomekeyring-1.0\n",
            "  python-secretstorage-doc python-setuptools-doc\n",
            "The following NEW packages will be installed:\n",
            "  autoconf automake autopoint autotools-dev debhelper dh-autoreconf\n",
            "  dh-strip-nondeterminism file gettext gettext-base gir1.2-atk-1.0\n",
            "  gir1.2-freedesktop gir1.2-gdkpixbuf-2.0 gir1.2-gtk-2.0 gir1.2-pango-1.0\n",
            "  intltool-debian libarchive-cpio-perl libarchive-zip-perl libatk1.0-dev\n",
            "  libcairo-script-interpreter2 libcairo2-dev libfile-stripnondeterminism-perl\n",
            "  libgail-common libgail18 libgdk-pixbuf2.0-dev libgtk2.0-0 libgtk2.0-bin\n",
            "  libgtk2.0-common libgtk2.0-dev libmagic-mgc libmagic1 libmail-sendmail-perl\n",
            "  libpango1.0-dev libpangoxft-1.0-0 libpixman-1-dev libsigsegv2\n",
            "  libsys-hostname-long-perl libtimedate-perl libtool libxcb-shm0-dev\n",
            "  libxcomposite-dev libxcursor-dev libxinerama-dev libxml2-utils libxrandr-dev\n",
            "  m4 po-debconf python-pip-whl python3-asn1crypto python3-cffi-backend\n",
            "  python3-crypto python3-cryptography python3-idna python3-keyring\n",
            "  python3-keyrings.alt python3-pip python3-pkg-resources python3-secretstorage\n",
            "  python3-setuptools python3-six python3-virtualenv python3-wheel python3-xdg\n",
            "  virtualenv x11proto-composite-dev x11proto-randr-dev x11proto-xinerama-dev\n",
            "0 upgraded, 67 newly installed, 0 to remove and 85 not upgraded.\n",
            "Need to get 14.1 MB of archives.\n",
            "After this operation, 66.0 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.4 [184 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.4 [68.6 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 file amd64 1:5.32-2ubuntu0.4 [22.1 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gettext-base amd64 0.19.8.1-6ubuntu0.3 [113 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsigsegv2 amd64 2.12-1 [14.7 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 m4 amd64 1.4.18-1 [197 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 autoconf all 2.69-11 [322 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 autotools-dev all 20180224.1 [39.6 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 automake all 1:1.15.1-3ubuntu2 [509 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 autopoint all 0.19.8.1-6ubuntu0.3 [426 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtool all 2.4.6-2 [194 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 dh-autoreconf all 17 [15.8 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libarchive-zip-perl all 1.60-1ubuntu0.1 [84.6 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfile-stripnondeterminism-perl all 0.040-1.1~build1 [13.8 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtimedate-perl all 2.3000-2 [37.5 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 dh-strip-nondeterminism all 0.040-1.1~build1 [5,208 B]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gettext amd64 0.19.8.1-6ubuntu0.3 [1,293 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 intltool-debian all 0.35.0+20060710.4 [24.9 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 po-debconf all 1.0.20 [232 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 debhelper all 11.1.6ubuntu2 [902 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic/main amd64 gir1.2-atk-1.0 amd64 2.28.1-1 [17.8 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic/main amd64 gir1.2-freedesktop amd64 1.56.1-1 [9,080 B]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic/main amd64 gir1.2-gdkpixbuf-2.0 amd64 2.36.11-2 [7,748 B]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgtk2.0-common all 2.24.32-1ubuntu1 [125 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpangoxft-1.0-0 amd64 1.40.14-1ubuntu0.1 [15.0 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gir1.2-pango-1.0 amd64 1.40.14-1ubuntu0.1 [21.6 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgtk2.0-0 amd64 2.24.32-1ubuntu1 [1,769 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu bionic/main amd64 gir1.2-gtk-2.0 amd64 2.24.32-1ubuntu1 [172 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu bionic/main amd64 libarchive-cpio-perl all 0.10-1 [9,644 B]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu bionic/main amd64 libatk1.0-dev amd64 2.28.1-1 [79.9 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcairo-script-interpreter2 amd64 1.15.10-2ubuntu0.1 [53.5 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu bionic/main amd64 libpixman-1-dev amd64 0.34.0-2 [244 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxcb-shm0-dev amd64 1.13-2~ubuntu18.04 [6,684 B]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcairo2-dev amd64 1.15.10-2ubuntu0.1 [626 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgail18 amd64 2.24.32-1ubuntu1 [14.2 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgail-common amd64 2.24.32-1ubuntu1 [112 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgdk-pixbuf2.0-dev amd64 2.36.11-2 [46.8 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgtk2.0-bin amd64 2.24.32-1ubuntu1 [7,536 B]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpango1.0-dev amd64 1.40.14-1ubuntu0.1 [288 kB]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-xinerama-dev all 2018.4-4 [2,628 B]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxinerama-dev amd64 2:1.1.3-1 [8,404 B]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-randr-dev all 2018.4-4 [2,620 B]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxrandr-dev amd64 2:1.5.1-1 [24.0 kB]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxcursor-dev amd64 1:1.1.15-1 [26.5 kB]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-composite-dev all 1:2018.4-4 [2,620 B]\n",
            "Get:46 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxcomposite-dev amd64 1:0.4.4-2 [9,136 B]\n",
            "Get:47 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxml2-utils amd64 2.9.4+dfsg1-6.1ubuntu1.3 [35.9 kB]\n",
            "Get:48 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgtk2.0-dev amd64 2.24.32-1ubuntu1 [2,652 kB]\n",
            "Get:49 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsys-hostname-long-perl all 1.5-1 [11.7 kB]\n",
            "Get:50 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmail-sendmail-perl all 0.80-1 [22.6 kB]\n",
            "Get:51 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python-pip-whl all 9.0.1-2.3~ubuntu1.18.04.4 [1,653 kB]\n",
            "Get:52 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-asn1crypto all 0.24.0-1 [72.8 kB]\n",
            "Get:53 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-cffi-backend amd64 1.11.5-1 [64.6 kB]\n",
            "Get:54 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-crypto amd64 2.6.1-8ubuntu2 [244 kB]\n",
            "Get:55 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-idna all 2.6-1 [32.5 kB]\n",
            "Get:56 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-six all 1.11.0-2 [11.4 kB]\n",
            "Get:57 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3-cryptography amd64 2.1.4-1ubuntu1.4 [220 kB]\n",
            "Get:58 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-secretstorage all 2.3.1-2 [12.1 kB]\n",
            "Get:59 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-keyring all 10.6.0-1 [26.7 kB]\n",
            "Get:60 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-keyrings.alt all 3.0-1 [16.6 kB]\n",
            "Get:61 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python3-pip all 9.0.1-2.3~ubuntu1.18.04.4 [114 kB]\n",
            "Get:62 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-pkg-resources all 39.0.1-2 [98.8 kB]\n",
            "Get:63 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-setuptools all 39.0.1-2 [248 kB]\n",
            "Get:64 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-virtualenv all 15.1.0+ds-1.1 [43.4 kB]\n",
            "Get:65 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-wheel all 0.30.0-0.2 [36.5 kB]\n",
            "Get:66 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3-xdg all 0.25-4ubuntu1.1 [31.3 kB]\n",
            "Get:67 http://archive.ubuntu.com/ubuntu bionic/universe amd64 virtualenv all 15.1.0+ds-1.1 [4,476 B]\n",
            "Fetched 14.1 MB in 5s (2,875 kB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Selecting previously unselected package libmagic-mgc.\n",
            "(Reading database ... 162099 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libmagic-mgc_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package libmagic1:amd64.\n",
            "Preparing to unpack .../01-libmagic1_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package file.\n",
            "Preparing to unpack .../02-file_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking file (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package gettext-base.\n",
            "Preparing to unpack .../03-gettext-base_0.19.8.1-6ubuntu0.3_amd64.deb ...\n",
            "Unpacking gettext-base (0.19.8.1-6ubuntu0.3) ...\n",
            "Selecting previously unselected package libsigsegv2:amd64.\n",
            "Preparing to unpack .../04-libsigsegv2_2.12-1_amd64.deb ...\n",
            "Unpacking libsigsegv2:amd64 (2.12-1) ...\n",
            "Selecting previously unselected package m4.\n",
            "Preparing to unpack .../05-m4_1.4.18-1_amd64.deb ...\n",
            "Unpacking m4 (1.4.18-1) ...\n",
            "Selecting previously unselected package autoconf.\n",
            "Preparing to unpack .../06-autoconf_2.69-11_all.deb ...\n",
            "Unpacking autoconf (2.69-11) ...\n",
            "Selecting previously unselected package autotools-dev.\n",
            "Preparing to unpack .../07-autotools-dev_20180224.1_all.deb ...\n",
            "Unpacking autotools-dev (20180224.1) ...\n",
            "Selecting previously unselected package automake.\n",
            "Preparing to unpack .../08-automake_1%3a1.15.1-3ubuntu2_all.deb ...\n",
            "Unpacking automake (1:1.15.1-3ubuntu2) ...\n",
            "Selecting previously unselected package autopoint.\n",
            "Preparing to unpack .../09-autopoint_0.19.8.1-6ubuntu0.3_all.deb ...\n",
            "Unpacking autopoint (0.19.8.1-6ubuntu0.3) ...\n",
            "Selecting previously unselected package libtool.\n",
            "Preparing to unpack .../10-libtool_2.4.6-2_all.deb ...\n",
            "Unpacking libtool (2.4.6-2) ...\n",
            "Selecting previously unselected package dh-autoreconf.\n",
            "Preparing to unpack .../11-dh-autoreconf_17_all.deb ...\n",
            "Unpacking dh-autoreconf (17) ...\n",
            "Selecting previously unselected package libarchive-zip-perl.\n",
            "Preparing to unpack .../12-libarchive-zip-perl_1.60-1ubuntu0.1_all.deb ...\n",
            "Unpacking libarchive-zip-perl (1.60-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libfile-stripnondeterminism-perl.\n",
            "Preparing to unpack .../13-libfile-stripnondeterminism-perl_0.040-1.1~build1_all.deb ...\n",
            "Unpacking libfile-stripnondeterminism-perl (0.040-1.1~build1) ...\n",
            "Selecting previously unselected package libtimedate-perl.\n",
            "Preparing to unpack .../14-libtimedate-perl_2.3000-2_all.deb ...\n",
            "Unpacking libtimedate-perl (2.3000-2) ...\n",
            "Selecting previously unselected package dh-strip-nondeterminism.\n",
            "Preparing to unpack .../15-dh-strip-nondeterminism_0.040-1.1~build1_all.deb ...\n",
            "Unpacking dh-strip-nondeterminism (0.040-1.1~build1) ...\n",
            "Selecting previously unselected package gettext.\n",
            "Preparing to unpack .../16-gettext_0.19.8.1-6ubuntu0.3_amd64.deb ...\n",
            "Unpacking gettext (0.19.8.1-6ubuntu0.3) ...\n",
            "Selecting previously unselected package intltool-debian.\n",
            "Preparing to unpack .../17-intltool-debian_0.35.0+20060710.4_all.deb ...\n",
            "Unpacking intltool-debian (0.35.0+20060710.4) ...\n",
            "Selecting previously unselected package po-debconf.\n",
            "Preparing to unpack .../18-po-debconf_1.0.20_all.deb ...\n",
            "Unpacking po-debconf (1.0.20) ...\n",
            "Selecting previously unselected package debhelper.\n",
            "Preparing to unpack .../19-debhelper_11.1.6ubuntu2_all.deb ...\n",
            "Unpacking debhelper (11.1.6ubuntu2) ...\n",
            "Selecting previously unselected package gir1.2-atk-1.0:amd64.\n",
            "Preparing to unpack .../20-gir1.2-atk-1.0_2.28.1-1_amd64.deb ...\n",
            "Unpacking gir1.2-atk-1.0:amd64 (2.28.1-1) ...\n",
            "Selecting previously unselected package gir1.2-freedesktop:amd64.\n",
            "Preparing to unpack .../21-gir1.2-freedesktop_1.56.1-1_amd64.deb ...\n",
            "Unpacking gir1.2-freedesktop:amd64 (1.56.1-1) ...\n",
            "Selecting previously unselected package gir1.2-gdkpixbuf-2.0:amd64.\n",
            "Preparing to unpack .../22-gir1.2-gdkpixbuf-2.0_2.36.11-2_amd64.deb ...\n",
            "Unpacking gir1.2-gdkpixbuf-2.0:amd64 (2.36.11-2) ...\n",
            "Selecting previously unselected package libgtk2.0-common.\n",
            "Preparing to unpack .../23-libgtk2.0-common_2.24.32-1ubuntu1_all.deb ...\n",
            "Unpacking libgtk2.0-common (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libpangoxft-1.0-0:amd64.\n",
            "Preparing to unpack .../24-libpangoxft-1.0-0_1.40.14-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libpangoxft-1.0-0:amd64 (1.40.14-1ubuntu0.1) ...\n",
            "Selecting previously unselected package gir1.2-pango-1.0:amd64.\n",
            "Preparing to unpack .../25-gir1.2-pango-1.0_1.40.14-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking gir1.2-pango-1.0:amd64 (1.40.14-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libgtk2.0-0:amd64.\n",
            "Preparing to unpack .../26-libgtk2.0-0_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgtk2.0-0:amd64 (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package gir1.2-gtk-2.0.\n",
            "Preparing to unpack .../27-gir1.2-gtk-2.0_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking gir1.2-gtk-2.0 (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libarchive-cpio-perl.\n",
            "Preparing to unpack .../28-libarchive-cpio-perl_0.10-1_all.deb ...\n",
            "Unpacking libarchive-cpio-perl (0.10-1) ...\n",
            "Selecting previously unselected package libatk1.0-dev:amd64.\n",
            "Preparing to unpack .../29-libatk1.0-dev_2.28.1-1_amd64.deb ...\n",
            "Unpacking libatk1.0-dev:amd64 (2.28.1-1) ...\n",
            "Selecting previously unselected package libcairo-script-interpreter2:amd64.\n",
            "Preparing to unpack .../30-libcairo-script-interpreter2_1.15.10-2ubuntu0.1_amd64.deb ...\n",
            "Unpacking libcairo-script-interpreter2:amd64 (1.15.10-2ubuntu0.1) ...\n",
            "Selecting previously unselected package libpixman-1-dev:amd64.\n",
            "Preparing to unpack .../31-libpixman-1-dev_0.34.0-2_amd64.deb ...\n",
            "Unpacking libpixman-1-dev:amd64 (0.34.0-2) ...\n",
            "Selecting previously unselected package libxcb-shm0-dev:amd64.\n",
            "Preparing to unpack .../32-libxcb-shm0-dev_1.13-2~ubuntu18.04_amd64.deb ...\n",
            "Unpacking libxcb-shm0-dev:amd64 (1.13-2~ubuntu18.04) ...\n",
            "Selecting previously unselected package libcairo2-dev:amd64.\n",
            "Preparing to unpack .../33-libcairo2-dev_1.15.10-2ubuntu0.1_amd64.deb ...\n",
            "Unpacking libcairo2-dev:amd64 (1.15.10-2ubuntu0.1) ...\n",
            "Selecting previously unselected package libgail18:amd64.\n",
            "Preparing to unpack .../34-libgail18_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgail18:amd64 (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libgail-common:amd64.\n",
            "Preparing to unpack .../35-libgail-common_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgail-common:amd64 (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libgdk-pixbuf2.0-dev.\n",
            "Preparing to unpack .../36-libgdk-pixbuf2.0-dev_2.36.11-2_amd64.deb ...\n",
            "Unpacking libgdk-pixbuf2.0-dev (2.36.11-2) ...\n",
            "Selecting previously unselected package libgtk2.0-bin.\n",
            "Preparing to unpack .../37-libgtk2.0-bin_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgtk2.0-bin (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libpango1.0-dev.\n",
            "Preparing to unpack .../38-libpango1.0-dev_1.40.14-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libpango1.0-dev (1.40.14-1ubuntu0.1) ...\n",
            "Selecting previously unselected package x11proto-xinerama-dev.\n",
            "Preparing to unpack .../39-x11proto-xinerama-dev_2018.4-4_all.deb ...\n",
            "Unpacking x11proto-xinerama-dev (2018.4-4) ...\n",
            "Selecting previously unselected package libxinerama-dev:amd64.\n",
            "Preparing to unpack .../40-libxinerama-dev_2%3a1.1.3-1_amd64.deb ...\n",
            "Unpacking libxinerama-dev:amd64 (2:1.1.3-1) ...\n",
            "Selecting previously unselected package x11proto-randr-dev.\n",
            "Preparing to unpack .../41-x11proto-randr-dev_2018.4-4_all.deb ...\n",
            "Unpacking x11proto-randr-dev (2018.4-4) ...\n",
            "Selecting previously unselected package libxrandr-dev:amd64.\n",
            "Preparing to unpack .../42-libxrandr-dev_2%3a1.5.1-1_amd64.deb ...\n",
            "Unpacking libxrandr-dev:amd64 (2:1.5.1-1) ...\n",
            "Selecting previously unselected package libxcursor-dev:amd64.\n",
            "Preparing to unpack .../43-libxcursor-dev_1%3a1.1.15-1_amd64.deb ...\n",
            "Unpacking libxcursor-dev:amd64 (1:1.1.15-1) ...\n",
            "Selecting previously unselected package x11proto-composite-dev.\n",
            "Preparing to unpack .../44-x11proto-composite-dev_1%3a2018.4-4_all.deb ...\n",
            "Unpacking x11proto-composite-dev (1:2018.4-4) ...\n",
            "Selecting previously unselected package libxcomposite-dev:amd64.\n",
            "Preparing to unpack .../45-libxcomposite-dev_1%3a0.4.4-2_amd64.deb ...\n",
            "Unpacking libxcomposite-dev:amd64 (1:0.4.4-2) ...\n",
            "Selecting previously unselected package libxml2-utils.\n",
            "Preparing to unpack .../46-libxml2-utils_2.9.4+dfsg1-6.1ubuntu1.3_amd64.deb ...\n",
            "Unpacking libxml2-utils (2.9.4+dfsg1-6.1ubuntu1.3) ...\n",
            "Selecting previously unselected package libgtk2.0-dev.\n",
            "Preparing to unpack .../47-libgtk2.0-dev_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgtk2.0-dev (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libsys-hostname-long-perl.\n",
            "Preparing to unpack .../48-libsys-hostname-long-perl_1.5-1_all.deb ...\n",
            "Unpacking libsys-hostname-long-perl (1.5-1) ...\n",
            "Selecting previously unselected package libmail-sendmail-perl.\n",
            "Preparing to unpack .../49-libmail-sendmail-perl_0.80-1_all.deb ...\n",
            "Unpacking libmail-sendmail-perl (0.80-1) ...\n",
            "Selecting previously unselected package python-pip-whl.\n",
            "Preparing to unpack .../50-python-pip-whl_9.0.1-2.3~ubuntu1.18.04.4_all.deb ...\n",
            "Unpacking python-pip-whl (9.0.1-2.3~ubuntu1.18.04.4) ...\n",
            "Selecting previously unselected package python3-asn1crypto.\n",
            "Preparing to unpack .../51-python3-asn1crypto_0.24.0-1_all.deb ...\n",
            "Unpacking python3-asn1crypto (0.24.0-1) ...\n",
            "Selecting previously unselected package python3-cffi-backend.\n",
            "Preparing to unpack .../52-python3-cffi-backend_1.11.5-1_amd64.deb ...\n",
            "Unpacking python3-cffi-backend (1.11.5-1) ...\n",
            "Selecting previously unselected package python3-crypto.\n",
            "Preparing to unpack .../53-python3-crypto_2.6.1-8ubuntu2_amd64.deb ...\n",
            "Unpacking python3-crypto (2.6.1-8ubuntu2) ...\n",
            "Selecting previously unselected package python3-idna.\n",
            "Preparing to unpack .../54-python3-idna_2.6-1_all.deb ...\n",
            "Unpacking python3-idna (2.6-1) ...\n",
            "Selecting previously unselected package python3-six.\n",
            "Preparing to unpack .../55-python3-six_1.11.0-2_all.deb ...\n",
            "Unpacking python3-six (1.11.0-2) ...\n",
            "Selecting previously unselected package python3-cryptography.\n",
            "Preparing to unpack .../56-python3-cryptography_2.1.4-1ubuntu1.4_amd64.deb ...\n",
            "Unpacking python3-cryptography (2.1.4-1ubuntu1.4) ...\n",
            "Selecting previously unselected package python3-secretstorage.\n",
            "Preparing to unpack .../57-python3-secretstorage_2.3.1-2_all.deb ...\n",
            "Unpacking python3-secretstorage (2.3.1-2) ...\n",
            "Selecting previously unselected package python3-keyring.\n",
            "Preparing to unpack .../58-python3-keyring_10.6.0-1_all.deb ...\n",
            "Unpacking python3-keyring (10.6.0-1) ...\n",
            "Selecting previously unselected package python3-keyrings.alt.\n",
            "Preparing to unpack .../59-python3-keyrings.alt_3.0-1_all.deb ...\n",
            "Unpacking python3-keyrings.alt (3.0-1) ...\n",
            "Selecting previously unselected package python3-pip.\n",
            "Preparing to unpack .../60-python3-pip_9.0.1-2.3~ubuntu1.18.04.4_all.deb ...\n",
            "Unpacking python3-pip (9.0.1-2.3~ubuntu1.18.04.4) ...\n",
            "Selecting previously unselected package python3-pkg-resources.\n",
            "Preparing to unpack .../61-python3-pkg-resources_39.0.1-2_all.deb ...\n",
            "Unpacking python3-pkg-resources (39.0.1-2) ...\n",
            "Selecting previously unselected package python3-setuptools.\n",
            "Preparing to unpack .../62-python3-setuptools_39.0.1-2_all.deb ...\n",
            "Unpacking python3-setuptools (39.0.1-2) ...\n",
            "Selecting previously unselected package python3-virtualenv.\n",
            "Preparing to unpack .../63-python3-virtualenv_15.1.0+ds-1.1_all.deb ...\n",
            "Unpacking python3-virtualenv (15.1.0+ds-1.1) ...\n",
            "Selecting previously unselected package python3-wheel.\n",
            "Preparing to unpack .../64-python3-wheel_0.30.0-0.2_all.deb ...\n",
            "Unpacking python3-wheel (0.30.0-0.2) ...\n",
            "Selecting previously unselected package python3-xdg.\n",
            "Preparing to unpack .../65-python3-xdg_0.25-4ubuntu1.1_all.deb ...\n",
            "Unpacking python3-xdg (0.25-4ubuntu1.1) ...\n",
            "Selecting previously unselected package virtualenv.\n",
            "Preparing to unpack .../66-virtualenv_15.1.0+ds-1.1_all.deb ...\n",
            "Unpacking virtualenv (15.1.0+ds-1.1) ...\n",
            "Setting up python-pip-whl (9.0.1-2.3~ubuntu1.18.04.4) ...\n",
            "Setting up libxcursor-dev:amd64 (1:1.1.15-1) ...\n",
            "Setting up gir1.2-atk-1.0:amd64 (2.28.1-1) ...\n",
            "Setting up libgtk2.0-common (2.24.32-1ubuntu1) ...\n",
            "Setting up libarchive-zip-perl (1.60-1ubuntu0.1) ...\n",
            "Setting up python3-cffi-backend (1.11.5-1) ...\n",
            "Setting up libtimedate-perl (2.3000-2) ...\n",
            "Setting up libcairo-script-interpreter2:amd64 (1.15.10-2ubuntu0.1) ...\n",
            "Setting up libsigsegv2:amd64 (2.12-1) ...\n",
            "Setting up python3-crypto (2.6.1-8ubuntu2) ...\n",
            "Setting up gir1.2-freedesktop:amd64 (1.56.1-1) ...\n",
            "Setting up python3-idna (2.6-1) ...\n",
            "Setting up python3-xdg (0.25-4ubuntu1.1) ...\n",
            "Setting up libxcb-shm0-dev:amd64 (1.13-2~ubuntu18.04) ...\n",
            "Setting up python3-six (1.11.0-2) ...\n",
            "Setting up libpangoxft-1.0-0:amd64 (1.40.14-1ubuntu0.1) ...\n",
            "Setting up libxml2-utils (2.9.4+dfsg1-6.1ubuntu1.3) ...\n",
            "Setting up libarchive-cpio-perl (0.10-1) ...\n",
            "Setting up gir1.2-gdkpixbuf-2.0:amd64 (2.36.11-2) ...\n",
            "Setting up libatk1.0-dev:amd64 (2.28.1-1) ...\n",
            "Setting up gettext-base (0.19.8.1-6ubuntu0.3) ...\n",
            "Setting up python3-wheel (0.30.0-0.2) ...\n",
            "Setting up m4 (1.4.18-1) ...\n",
            "Setting up python3-pkg-resources (39.0.1-2) ...\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Setting up gir1.2-pango-1.0:amd64 (1.40.14-1ubuntu0.1) ...\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Setting up python3-asn1crypto (0.24.0-1) ...\n",
            "Setting up libsys-hostname-long-perl (1.5-1) ...\n",
            "Setting up libgdk-pixbuf2.0-dev (2.36.11-2) ...\n",
            "Setting up libmail-sendmail-perl (0.80-1) ...\n",
            "Setting up x11proto-xinerama-dev (2018.4-4) ...\n",
            "Setting up autotools-dev (20180224.1) ...\n",
            "Setting up libpixman-1-dev:amd64 (0.34.0-2) ...\n",
            "Setting up x11proto-randr-dev (2018.4-4) ...\n",
            "Setting up libxinerama-dev:amd64 (2:1.1.3-1) ...\n",
            "Setting up python3-virtualenv (15.1.0+ds-1.1) ...\n",
            "Setting up python3-pip (9.0.1-2.3~ubuntu1.18.04.4) ...\n",
            "Setting up python3-setuptools (39.0.1-2) ...\n",
            "Setting up virtualenv (15.1.0+ds-1.1) ...\n",
            "Setting up x11proto-composite-dev (1:2018.4-4) ...\n",
            "Setting up autopoint (0.19.8.1-6ubuntu0.3) ...\n",
            "Setting up libfile-stripnondeterminism-perl (0.040-1.1~build1) ...\n",
            "Setting up python3-cryptography (2.1.4-1ubuntu1.4) ...\n",
            "Setting up libgtk2.0-0:amd64 (2.24.32-1ubuntu1) ...\n",
            "Setting up libgail18:amd64 (2.24.32-1ubuntu1) ...\n",
            "Setting up libxrandr-dev:amd64 (2:1.5.1-1) ...\n",
            "Setting up libcairo2-dev:amd64 (1.15.10-2ubuntu0.1) ...\n",
            "Setting up gettext (0.19.8.1-6ubuntu0.3) ...\n",
            "Setting up libxcomposite-dev:amd64 (1:0.4.4-2) ...\n",
            "Setting up python3-keyrings.alt (3.0-1) ...\n",
            "Setting up libgail-common:amd64 (2.24.32-1ubuntu1) ...\n",
            "Setting up autoconf (2.69-11) ...\n",
            "Setting up file (1:5.32-2ubuntu0.4) ...\n",
            "Setting up intltool-debian (0.35.0+20060710.4) ...\n",
            "Setting up automake (1:1.15.1-3ubuntu2) ...\n",
            "update-alternatives: using /usr/bin/automake-1.15 to provide /usr/bin/automake (automake) in auto mode\n",
            "Setting up libpango1.0-dev (1.40.14-1ubuntu0.1) ...\n",
            "Setting up python3-secretstorage (2.3.1-2) ...\n",
            "Setting up gir1.2-gtk-2.0 (2.24.32-1ubuntu1) ...\n",
            "Setting up libgtk2.0-bin (2.24.32-1ubuntu1) ...\n",
            "Setting up libtool (2.4.6-2) ...\n",
            "Setting up po-debconf (1.0.20) ...\n",
            "Setting up python3-keyring (10.6.0-1) ...\n",
            "Setting up libgtk2.0-dev (2.24.32-1ubuntu1) ...\n",
            "Setting up dh-autoreconf (17) ...\n",
            "Setting up debhelper (11.1.6ubuntu2) ...\n",
            "Setting up dh-strip-nondeterminism (0.040-1.1~build1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libpng-dev is already the newest version (1.6.34-1ubuntu0.18.04.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 85 not upgraded.\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from -r /opt/intel/openvino/deployment_tools/demo/../open_model_zoo/tools/downloader/requirements.in (line 1)) (3.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from -r /opt/intel/openvino/deployment_tools/demo/../open_model_zoo/tools/downloader/requirements.in (line 2)) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->-r /opt/intel/openvino/deployment_tools/demo/../open_model_zoo/tools/downloader/requirements.in (line 2)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->-r /opt/intel/openvino/deployment_tools/demo/../open_model_zoo/tools/downloader/requirements.in (line 2)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->-r /opt/intel/openvino/deployment_tools/demo/../open_model_zoo/tools/downloader/requirements.in (line 2)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->-r /opt/intel/openvino/deployment_tools/demo/../open_model_zoo/tools/downloader/requirements.in (line 2)) (2020.12.5)\n",
            "Run python3 /opt/intel/openvino_2020.1.023/deployment_tools/open_model_zoo/tools/downloader/downloader.py --name squeezenet1.1 --output_dir /root/openvino_models/models --cache_dir /root/openvino_models/cache\n",
            "\n",
            "################|| Downloading models ||################\n",
            "\n",
            "========== Downloading /root/openvino_models/models/public/squeezenet1.1/squeezenet1.1.prototxt\n",
            "... 100%, 9 KB, 40308 KB/s, 0 seconds passed\n",
            "\n",
            "========== Downloading /root/openvino_models/models/public/squeezenet1.1/squeezenet1.1.caffemodel\n",
            "... 100%, 4834 KB, 6299 KB/s, 0 seconds passed\n",
            "\n",
            "################|| Post-processing ||################\n",
            "\n",
            "========== Replacing text in /root/openvino_models/models/public/squeezenet1.1/squeezenet1.1.prototxt\n",
            "\n",
            "\n",
            "###################################################\n",
            "\n",
            "Install Model Optimizer dependencies\n",
            "\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Fetched 252 kB in 2s (131 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "85 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python3-pip is already the newest version (9.0.1-2.3~ubuntu1.18.04.4).\n",
            "The following additional packages will be installed:\n",
            "  gcc-6-base python3.6-venv\n",
            "The following NEW packages will be installed:\n",
            "  gcc-6-base libgfortran3 python3-venv python3.6-venv\n",
            "0 upgraded, 4 newly installed, 0 to remove and 85 not upgraded.\n",
            "Need to get 294 kB of archives.\n",
            "After this operation, 1,438 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 gcc-6-base amd64 6.5.0-2ubuntu1~18.04 [16.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libgfortran3 amd64 6.5.0-2ubuntu1~18.04 [270 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python3.6-venv amd64 3.6.9-1~18.04ubuntu1.4 [6,188 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python3-venv amd64 3.6.7-1~18.04 [1,208 B]\n",
            "Fetched 294 kB in 2s (158 kB/s)\n",
            "Selecting previously unselected package gcc-6-base:amd64.\n",
            "(Reading database ... 164840 files and directories currently installed.)\n",
            "Preparing to unpack .../gcc-6-base_6.5.0-2ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking gcc-6-base:amd64 (6.5.0-2ubuntu1~18.04) ...\n",
            "Selecting previously unselected package libgfortran3:amd64.\n",
            "Preparing to unpack .../libgfortran3_6.5.0-2ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking libgfortran3:amd64 (6.5.0-2ubuntu1~18.04) ...\n",
            "Selecting previously unselected package python3.6-venv.\n",
            "Preparing to unpack .../python3.6-venv_3.6.9-1~18.04ubuntu1.4_amd64.deb ...\n",
            "Unpacking python3.6-venv (3.6.9-1~18.04ubuntu1.4) ...\n",
            "Selecting previously unselected package python3-venv.\n",
            "Preparing to unpack .../python3-venv_3.6.7-1~18.04_amd64.deb ...\n",
            "Unpacking python3-venv (3.6.7-1~18.04) ...\n",
            "Setting up gcc-6-base:amd64 (6.5.0-2ubuntu1~18.04) ...\n",
            "Setting up python3.6-venv (3.6.9-1~18.04ubuntu1.4) ...\n",
            "Setting up python3-venv (3.6.7-1~18.04) ...\n",
            "Setting up libgfortran3:amd64 (6.5.0-2ubuntu1~18.04) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.7/dist-packages (from -r /opt/intel/openvino_2020.1.023/deployment_tools/model_optimizer/install_prerequisites/../requirements_caffe.txt (line 1)) (2.5.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from -r /opt/intel/openvino_2020.1.023/deployment_tools/model_optimizer/install_prerequisites/../requirements_caffe.txt (line 2)) (1.17.5)\n",
            "Collecting protobuf==3.6.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/30/289ead101f94998d88e8961a3548aea29417ae0057be23972483cddebf4f/protobuf-3.6.1-cp37-cp37m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     || 1.1MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: defusedxml>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from -r /opt/intel/openvino_2020.1.023/deployment_tools/model_optimizer/install_prerequisites/../requirements_caffe.txt (line 4)) (0.7.1)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=1.11->-r /opt/intel/openvino_2020.1.023/deployment_tools/model_optimizer/install_prerequisites/../requirements_caffe.txt (line 1)) (4.4.2)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf==3.6.1->-r /opt/intel/openvino_2020.1.023/deployment_tools/model_optimizer/install_prerequisites/../requirements_caffe.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf==3.6.1->-r /opt/intel/openvino_2020.1.023/deployment_tools/model_optimizer/install_prerequisites/../requirements_caffe.txt (line 3)) (56.1.0)\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement numpy~=1.19.2, but you'll have numpy 1.17.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement protobuf>=3.9.2, but you'll have protobuf 3.6.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-metadata 1.0.0 has requirement protobuf<4,>=3.7, but you'll have protobuf 3.6.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-hub 0.12.0 has requirement protobuf>=3.8.0, but you'll have protobuf 3.6.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kapre 0.3.5 has requirement numpy>=1.18.5, but you'll have numpy 1.17.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: googleapis-common-protos 1.53.0 has requirement protobuf>=3.12.0, but you'll have protobuf 3.6.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-api-core 1.26.3 has requirement protobuf>=3.12.0, but you'll have protobuf 3.6.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: protobuf\n",
            "  Found existing installation: protobuf 3.12.4\n",
            "    Uninstalling protobuf-3.12.4:\n",
            "      Successfully uninstalled protobuf-3.12.4\n",
            "Successfully installed protobuf-3.6.1\n",
            "[WARNING] All Model Optimizer dependencies are installed globally.\n",
            "[WARNING] If you want to keep Model Optimizer in separate sandbox\n",
            "[WARNING] run install_prerequisites.sh venv {caffe|tf|mxnet|kaldi|onnx}\n",
            "\n",
            "\n",
            "###################################################\n",
            "\n",
            "Convert a model with Model Optimizer\n",
            "\n",
            "Run python3 /opt/intel/openvino_2020.1.023/deployment_tools/open_model_zoo/tools/downloader/converter.py --mo /opt/intel/openvino_2020.1.023/deployment_tools/model_optimizer/mo.py --name squeezenet1.1 -d /root/openvino_models/models -o /root/openvino_models/ir --precisions FP16\n",
            "\n",
            "========= Converting squeezenet1.1 to IR (FP16)\n",
            "Conversion command: /usr/bin/python3 -- /opt/intel/openvino_2020.1.023/deployment_tools/model_optimizer/mo.py --framework=caffe --data_type=FP16 --output_dir=/root/openvino_models/ir/public/squeezenet1.1/FP16 --model_name=squeezenet1.1 '--input_shape=[1,3,227,227]' --input=data '--mean_values=data[104.0,117.0,123.0]' --output=prob --input_model=/root/openvino_models/models/public/squeezenet1.1/squeezenet1.1.caffemodel --input_proto=/root/openvino_models/models/public/squeezenet1.1/squeezenet1.1.prototxt\n",
            "\n",
            "Model Optimizer arguments:\n",
            "Common parameters:\n",
            "\t- Path to the Input Model: \t/root/openvino_models/models/public/squeezenet1.1/squeezenet1.1.caffemodel\n",
            "\t- Path for generated IR: \t/root/openvino_models/ir/public/squeezenet1.1/FP16\n",
            "\t- IR output name: \tsqueezenet1.1\n",
            "\t- Log level: \tERROR\n",
            "\t- Batch: \tNot specified, inherited from the model\n",
            "\t- Input layers: \tdata\n",
            "\t- Output layers: \tprob\n",
            "\t- Input shapes: \t[1,3,227,227]\n",
            "\t- Mean values: \tdata[104.0,117.0,123.0]\n",
            "\t- Scale values: \tNot specified\n",
            "\t- Scale factor: \tNot specified\n",
            "\t- Precision of IR: \tFP16\n",
            "\t- Enable fusing: \tTrue\n",
            "\t- Enable grouped convolutions fusing: \tTrue\n",
            "\t- Move mean values to preprocess section: \tFalse\n",
            "\t- Reverse input channels: \tFalse\n",
            "Caffe specific parameters:\n",
            "\t- Path to Python Caffe* parser generated from caffe.proto: \t/opt/intel/openvino_2020.1.023/deployment_tools/model_optimizer/mo/front/caffe/proto\n",
            "\t- Enable resnet optimization: \tTrue\n",
            "\t- Path to the Input prototxt: \t/root/openvino_models/models/public/squeezenet1.1/squeezenet1.1.prototxt\n",
            "\t- Path to CustomLayersMapping.xml: \tDefault\n",
            "\t- Path to a mean file: \tNot specified\n",
            "\t- Offsets for a mean file: \tNot specified\n",
            "Model Optimizer version: \t2020.1.0-61-gd349c3ba4a\n",
            "\n",
            "[ SUCCESS ] Generated IR version 10 model.\n",
            "[ SUCCESS ] XML file: /root/openvino_models/ir/public/squeezenet1.1/FP16/squeezenet1.1.xml\n",
            "[ SUCCESS ] BIN file: /root/openvino_models/ir/public/squeezenet1.1/FP16/squeezenet1.1.bin\n",
            "[ SUCCESS ] Total execution time: 5.49 seconds. \n",
            "[ SUCCESS ] Memory consumed: 112 MB. \n",
            "\n",
            "\n",
            "\n",
            "###################################################\n",
            "\n",
            "Build Inference Engine samples\n",
            "\n",
            "-- The C compiler identification is GNU 7.5.0\n",
            "-- The CXX compiler identification is GNU 7.5.0\n",
            "-- Check for working C compiler: /usr/bin/cc\n",
            "-- Check for working C compiler: /usr/bin/cc -- works\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++\n",
            "-- Check for working CXX compiler: /usr/bin/c++ -- works\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Looking for C++ include unistd.h\n",
            "-- Looking for C++ include unistd.h - found\n",
            "-- Looking for C++ include stdint.h\n",
            "-- Looking for C++ include stdint.h - found\n",
            "-- Looking for C++ include sys/types.h\n",
            "-- Looking for C++ include sys/types.h - found\n",
            "-- Looking for C++ include fnmatch.h\n",
            "-- Looking for C++ include fnmatch.h - found\n",
            "-- Looking for strtoll\n",
            "-- Looking for strtoll - found\n",
            "-- Found InferenceEngine: /opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64/libinference_engine.so (Required is at least version \"2.1\") \n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /root/inference_engine_samples_build\n",
            "\u001b[35m\u001b[1mScanning dependencies of target format_reader\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target gflags_nothreads_static\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object thirdparty/gflags/CMakeFiles/gflags_nothreads_static.dir/src/gflags.cc.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object thirdparty/gflags/CMakeFiles/gflags_nothreads_static.dir/src/gflags_reporting.cc.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object thirdparty/gflags/CMakeFiles/gflags_nothreads_static.dir/src/gflags_completions.cc.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object common/format_reader/CMakeFiles/format_reader.dir/bmp.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object common/format_reader/CMakeFiles/format_reader.dir/format_reader.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object common/format_reader/CMakeFiles/format_reader.dir/MnistUbyte.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object common/format_reader/CMakeFiles/format_reader.dir/opencv_wraper.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX shared library ../../intel64/Release/lib/libformat_reader.so\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX static library ../../intel64/Release/lib/libgflags_nothreads.a\u001b[0m\n",
            "[ 81%] Built target gflags_nothreads_static\n",
            "[ 81%] Built target format_reader\n",
            "\u001b[35m\u001b[1mScanning dependencies of target classification_sample_async\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object classification_sample_async/CMakeFiles/classification_sample_async.dir/main.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../intel64/Release/classification_sample_async\u001b[0m\n",
            "[100%] Built target classification_sample_async\n",
            "\n",
            "\n",
            "###################################################\n",
            "\n",
            "Run Inference Engine classification sample\n",
            "\n",
            "Run ./classification_sample_async -d CPU -i /opt/intel/openvino/deployment_tools/demo/car.png -m /root/openvino_models/ir/public/squeezenet1.1/FP16/squeezenet1.1.xml\n",
            "\n",
            "[ INFO ] InferenceEngine: \n",
            "\tAPI version ............ 2.1\n",
            "\tBuild .................. 37988\n",
            "\tDescription ....... API\n",
            "[ INFO ] Parsing input parameters\n",
            "[ INFO ] Parsing input parameters\n",
            "[ INFO ] Files were added: 1\n",
            "[ INFO ]     /opt/intel/openvino/deployment_tools/demo/car.png\n",
            "[ INFO ] Creating Inference Engine\n",
            "\tCPU\n",
            "\tMKLDNNPlugin version ......... 2.1\n",
            "\tBuild ........... 37988\n",
            "\n",
            "[ INFO ] Loading network files\n",
            "[ INFO ] Preparing input blobs\n",
            "[ WARNING ] Image is resized from (787, 259) to (227, 227)\n",
            "[ INFO ] Batch size is 1\n",
            "[ INFO ] Loading model to the device\n",
            "[ INFO ] Create infer request\n",
            "[ INFO ] Start inference (10 asynchronous executions)\n",
            "[ INFO ] Completed 1 async request execution\n",
            "[ INFO ] Completed 2 async request execution\n",
            "[ INFO ] Completed 3 async request execution\n",
            "[ INFO ] Completed 4 async request execution\n",
            "[ INFO ] Completed 5 async request execution\n",
            "[ INFO ] Completed 6 async request execution\n",
            "[ INFO ] Completed 7 async request execution\n",
            "[ INFO ] Completed 8 async request execution\n",
            "[ INFO ] Completed 9 async request execution\n",
            "[ INFO ] Completed 10 async request execution\n",
            "[ INFO ] Processing output blobs\n",
            "\n",
            "Top 10 results:\n",
            "\n",
            "Image /opt/intel/openvino/deployment_tools/demo/car.png\n",
            "\n",
            "classid probability label\n",
            "------- ----------- -----\n",
            "817     0.6853030   sports car, sport car\n",
            "479     0.1835197   car wheel\n",
            "511     0.0917197   convertible\n",
            "436     0.0200694   beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon\n",
            "751     0.0069604   racer, race car, racing car\n",
            "656     0.0044177   minivan\n",
            "717     0.0024739   pickup, pickup truck\n",
            "581     0.0017788   grille, radiator grille\n",
            "468     0.0013083   cab, hack, taxi, taxicab\n",
            "661     0.0007443   Model T\n",
            "\n",
            "[ INFO ] Execution successful\n",
            "\n",
            "[ INFO ] This sample is an API example, for any performance measurements please use the dedicated benchmark_app tool\n",
            "\n",
            "\n",
            "###################################################\n",
            "\n",
            "Demo completed successfully.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TygLw4WIO8BD"
      },
      "source": [
        "### Here we run some modifications in the ssd2 OpenVINO extension for TF so that our Mobilenet SSDv2 model can convert successfully to the IR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-bGZHGcMNbZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7867f54a-a2c7-4f88-907b-1cb48323c5e1"
      },
      "source": [
        "%cd /opt/intel/openvino/deployment_tools/model_optimizer/extensions/front/tf/\n",
        "#openvino fixes: edit \n",
        "# Read in the file, make sure the .json corresponds to the model!!!\n",
        "with open('ssd_v2_support.json', 'r') as file :\n",
        "  filedata = file.read()\n",
        "\n",
        "# Replace the target string\n",
        "filedata = filedata.replace('\"Postprocessor/ToFloat\"', '\"Postprocessor/Cast_1\"')\n",
        "\n",
        "# Write the file out again\n",
        "with open('ssd_v2_support.json', 'w') as file:\n",
        "  file.write(filedata)\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/opt/intel/openvino_2020.1.023/deployment_tools/model_optimizer/extensions/front/tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROig0wlCkxoX"
      },
      "source": [
        "%cp \"/content/models/research/fine_tuned_model/saved_model/saved_model.pb\" \"/content/models/research/fine_tuned_model/\""
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7sqTX79W6hh"
      },
      "source": [
        "## Convert TF model to Open Vino Intermediate Representation\n",
        "If using own model, please change to your desired name for output directory --output_dir \"choose name\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsWggE5AIWS6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e6173e3-c372-4a56-f715-75731d26c6f7"
      },
      "source": [
        "#CONVERT TF MODEL to OPEN VINO IRv10. saved in IR_V10_fruits_mnssdv2_6k directory or\n",
        "#choose own name for --output_dir \"choose name\"\n",
        "%cd \"/content/models/research/fine_tuned_model/\"\n",
        "!source /opt/intel/openvino/bin/setupvars.sh && \\\n",
        "    python /opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n",
        "    --input_model frozen_inference_graph.pb \\\n",
        "    --tensorflow_use_custom_operations_config /opt/intel/openvino/deployment_tools/model_optimizer/extensions/front/tf/ssd_v2_support.json \\\n",
        "    --tensorflow_object_detection_api_pipeline_config pipeline.config \\\n",
        "    --reverse_input_channels \\\n",
        "    --output_dir signs \\\n",
        "    --data_type FP16"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research/fine_tuned_model\n",
            "[setupvars.sh] OpenVINO environment initialized\n",
            "[ WARNING ]  Use of deprecated cli option --tensorflow_use_custom_operations_config detected. Option use in the following releases will be fatal. Please use --transformations_config cli option instead\n",
            "Model Optimizer arguments:\n",
            "Common parameters:\n",
            "\t- Path to the Input Model: \t/content/models/research/fine_tuned_model/frozen_inference_graph.pb\n",
            "\t- Path for generated IR: \t/content/models/research/fine_tuned_model/signs\n",
            "\t- IR output name: \tfrozen_inference_graph\n",
            "\t- Log level: \tERROR\n",
            "\t- Batch: \tNot specified, inherited from the model\n",
            "\t- Input layers: \tNot specified, inherited from the model\n",
            "\t- Output layers: \tNot specified, inherited from the model\n",
            "\t- Input shapes: \tNot specified, inherited from the model\n",
            "\t- Mean values: \tNot specified\n",
            "\t- Scale values: \tNot specified\n",
            "\t- Scale factor: \tNot specified\n",
            "\t- Precision of IR: \tFP16\n",
            "\t- Enable fusing: \tTrue\n",
            "\t- Enable grouped convolutions fusing: \tTrue\n",
            "\t- Move mean values to preprocess section: \tFalse\n",
            "\t- Reverse input channels: \tTrue\n",
            "TensorFlow specific parameters:\n",
            "\t- Input model in text protobuf format: \tFalse\n",
            "\t- Path to model dump for TensorBoard: \tNone\n",
            "\t- List of shared libraries with TensorFlow custom layers implementation: \tNone\n",
            "\t- Update the configuration file with input/output node names: \tNone\n",
            "\t- Use configuration file used to generate the model with Object Detection API: \t/content/models/research/fine_tuned_model/pipeline.config\n",
            "\t- Operations to offload: \tNone\n",
            "\t- Patterns to offload: \tNone\n",
            "\t- Use the config file: \t/opt/intel/openvino/deployment_tools/model_optimizer/extensions/front/tf/ssd_v2_support.json\n",
            "Model Optimizer version: \t2020.1.0-61-gd349c3ba4a\n",
            "The Preprocessor block has been removed. Only nodes performing mean value subtraction and scaling (if applicable) are kept.\n",
            "\n",
            "[ SUCCESS ] Generated IR version 10 model.\n",
            "[ SUCCESS ] XML file: /content/models/research/fine_tuned_model/signs/frozen_inference_graph.xml\n",
            "[ SUCCESS ] BIN file: /content/models/research/fine_tuned_model/signs/frozen_inference_graph.bin\n",
            "[ SUCCESS ] Total execution time: 36.36 seconds. \n",
            "[ SUCCESS ] Memory consumed: 1284 MB. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9R7LTkNvZos",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cdba9d4-92b6-4d00-a796-db8eda79f229"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P15Hl-Fbmkib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0c3105c-cdd3-4954-a7f9-cc23854770a7"
      },
      "source": [
        "#check directory containing the exported TF trained model and the IRv10 folder\n",
        "%ls "
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint                      model.ckpt.index  \u001b[0m\u001b[01;34msaved_model\u001b[0m/\n",
            "frozen_inference_graph.pb       model.ckpt.meta   saved_model.pb\n",
            "model.ckpt.data-00000-of-00001  pipeline.config   \u001b[01;34msigns\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD2BMtXoPjYN"
      },
      "source": [
        "## Now we compile the IR model to a .blob for use on DepthAI modules/platform\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqZqajTIP3Sv"
      },
      "source": [
        "### We save the blob in the IR directory from above, corresponding to --output_dir parameter above. \n",
        "The blob filename will be *frozen_inference_graph.blob*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBDt-QKvlcdO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef3bdc5a-4afd-4a10-9dd0-bef1e418d3ea"
      },
      "source": [
        "%ls //content/models/research/fine_tuned_model/signs"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "frozen_inference_graph.bin      frozen_inference_graph.xml\n",
            "frozen_inference_graph.mapping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqiPpnCwPioH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "9459e95a-b789-4aef-93d0-23e54b1fe6f4"
      },
      "source": [
        "#No changes needed here unless using custom data.\n",
        "#CHOOSE the directory where you would like to save the blob.\n",
        "# I use the same --output_dir as above for the IR conversion\n",
        "blob_dir = \"/content/models/research/fine_tuned_model/signs\"\n",
        "\n",
        "#Copy the path of your .xml and .bin files. For that, you can look at the IR\n",
        "#conversion output cell, select and copy from:\n",
        "#[SUCCESS] XML file and bin file paths.\n",
        "#Or you can choose to compile other .xml .bin files from a different location\n",
        "#\n",
        "xmlfile = \"/content/models/research/fine_tuned_model/signs/frozen_inference_graph.xml\"\n",
        "binfile = \"//content/models/research/fine_tuned_model/signs/frozen_inference_graph.bin\"\n",
        "\n",
        "import requests\n",
        "\n",
        "#For openvino 20.01 use this link to compile the blob\n",
        "url = \"http://69.164.214.171:8080\"\n",
        "\n",
        "\n",
        "#open vino 20.02 link:\n",
        "# url = \"69.164.214.171:8081\"\n",
        "\n",
        "payload = {'compiler_params': '-ip U8 -VPU_MYRIAD_PLATFORM VPU_MYRIAD_2480 -VPU_NUMBER_OF_SHAVES 4 -VPU_NUMBER_OF_CMX_SLICES 4'}\n",
        "files = [\n",
        "  ('definition', open(xmlfile,'rb')),\n",
        "  ('weights', open(binfile,'rb'))\n",
        "]\n",
        "# headers = {\n",
        "#   'Content-Type': 'application/json'\n",
        "# }\n",
        "response = requests.request(\"POST\", url, data = payload, files = files)\n",
        "blobnameraw = response.headers.get('Content-Disposition')\n",
        "print(blobnameraw)\n",
        "blobname = blobnameraw[blobnameraw.find('='):][1:]\n",
        "with open(blob_dir + blobname, 'wb') as f:\n",
        "  f.write(response.content)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ConnectionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1276\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1277\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1322\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1271\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1272\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1070\u001b[0m                         \u001b[0;34m+\u001b[0m \u001b[0;34mb'\\r\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 993\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    637\u001b[0m             retries = retries.increment(method, url, error=e, _pool=self,\n\u001b[0;32m--> 638\u001b[0;31m                                         _stacktrace=sys.exc_info()[2])\n\u001b[0m\u001b[1;32m    639\u001b[0m             \u001b[0mretries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_method_retryable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1276\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1277\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1322\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1271\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1272\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1070\u001b[0m                         \u001b[0;34m+\u001b[0m \u001b[0;34mb'\\r\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 993\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mProtocolError\u001b[0m: ('Connection aborted.', BrokenPipeError(32, 'Broken pipe'))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-c682f56ba08f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#   'Content-Type': 'application/json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# }\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mblobnameraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Content-Disposition'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblobnameraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    528\u001b[0m         }\n\u001b[1;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mProtocolError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mMaxRetryError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionError\u001b[0m: ('Connection aborted.', BrokenPipeError(32, 'Broken pipe'))"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qJ0knjImLIn"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('/content/models/research/fine_tuned_model/signs/frozen_inference_graph.blob')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24-2kaAFvyEd"
      },
      "source": [
        "## To run the .blob in DepthAI, we proceed step 5 here:\n",
        " https://docs.luxonis.com/tutorials/object_det_mnssv2_training/"
      ]
    }
  ]
}